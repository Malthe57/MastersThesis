{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to train MIMO and Naive models with $M=2$ subnetworks using $K=10$ repititions to see how the variance generalises across repititions, as described by Havasi et al. in their explainer notebook. \n",
    "This means that we train $K=10$ versions of the MIMO models (on different sampled data) and then evaluate what they describe as the expected mean squared error \n",
    "$$\n",
    "\\mathcal{E}_M = \\mathbb{E}_{\\bm{x}',y' \\in \\mathbb{X}_{\\text{test}}} [\\mathbb{E}_{\\mathbb{X}}[(\\hat{f}_M(\\bm{x}',...,\\bm{x}')- y')^2]]\n",
    "$$\n",
    "Through bias-variance decomposition, we can get the variance as:\n",
    "$$\n",
    "\\mathbb{E}_{\\bm{x}',y' \\in \\mathbb{X}_{\\text{test}}} [\\mathbb{E}_{\\mathbb{X}}[(\\bar{f}_M(\\bm{x}',...,\\bm{x}')- \\hat{f}_M(\\bm{x}',...,\\bm{x}'))^2]]\n",
    "$$\n",
    "where $\\hat{f}_M$ is a regressor with $M$ subnetworks and $\\bar{f}_M = \\frac{1}{K} \\sum_{k=1}^K \\hat{f}(x)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions ðŸ¤–\n",
    "\n",
    "def train(model, optimizer, trainloader, valloader, epochs=500, model_name='MIMO', val_every_n_epochs=10):\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_,y_ = x_.float(), y_.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, individual_outputs = model(x_)\n",
    "            loss = nn.functional.mse_loss(individual_outputs, y_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())  \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float(), val_y.float()\n",
    "                    val_output, val_individual_outputs = model(val_x)\n",
    "                    val_loss = nn.functional.mse_loss(val_individual_outputs, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, val_losses\n",
    "\n",
    "def get_train_val_dataloaders(N_train=500, N_val=200, is_naive=False):\n",
    "    # Generate train data\n",
    "    x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "    # Generate validation data\n",
    "    x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "    # make dataset and get dataloaders\n",
    "    traindata = ToyDataset(x, y)\n",
    "    valdata = ToyDataset(x_val, y_val)\n",
    "    \n",
    "    if is_naive:\n",
    "        trainloader = DataLoader(traindata, batch_size=60, shuffle=True, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "        valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "    else:\n",
    "        trainloader = DataLoader(traindata, batch_size=60*M, shuffle=True, collate_fn=lambda x: train_collate_fn(x, M), drop_last=True)\n",
    "        valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)  \n",
    "    \n",
    "    return trainloader, valloader\n",
    "\n",
    "\n",
    "\n",
    "def train_K_repitions(M, epochs=500, val_every_n_epochs=10, repititions=20, is_naive=False):\n",
    "\n",
    "    K_losses = {}\n",
    "    K_val_losses = {}\n",
    "\n",
    "    for k in tqdm(range(repititions)):\n",
    "\n",
    "        model = NaiveNetwork(n_subnetworks=M) if is_naive else MIMONetwork(n_subnetworks=M)\n",
    "        name = 'naive_models/Naive' if is_naive else 'mimo_models/MIMO'\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        \n",
    "        trainloader, valloader = get_train_val_dataloaders(N_train, N_val, is_naive=is_naive)\n",
    "\n",
    "        losses, val_losses = train(model, optimizer, trainloader, valloader, epochs=epochs, model_name=f'{name}_{k}', val_every_n_epochs=val_every_n_epochs)\n",
    "\n",
    "        K_losses[k] = losses\n",
    "        K_val_losses[k] = val_losses\n",
    "\n",
    "    return K_losses, K_val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MIMO model\n",
    "M = 2\n",
    "# number of repititions\n",
    "K = 10\n",
    "is_naive = False\n",
    "\n",
    "K_losses, K_val_losses = train_K_repitions(M, epochs=5000, val_every_n_epochs=2, repititions=K, is_naive=is_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss for one of the repititions\n",
    "plot_loss(K_losses[0], K_val_losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Naive model\n",
    "M = 2\n",
    "# number of repititions\n",
    "K = 10\n",
    "is_naive = True\n",
    "\n",
    "K_losses, K_val_losses = train_K_repitions(M, epochs=500, val_every_n_epochs=2, repititions=K, is_naive=is_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss for one of the repititions\n",
    "plot_loss(K_losses[0], K_val_losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "naive_dir = \"naive_models\"\n",
    "mimo_dir = \"mimo_models\"\n",
    "\n",
    "naive_models = [os.path.join(naive_dir, f) for f in os.listdir(naive_dir)]\n",
    "mimo_models = [os.path.join(mimo_dir, f) for f in os.listdir(mimo_dir)]\n",
    "\n",
    "def rep_inference(models, testloader)\n",
    "    f_hats = np.zeros((K, N_test))\n",
    "\n",
    "    for i, mod in enumerate(models):\n",
    "\n",
    "        for test_x, test_y in testloader:\n",
    "\n",
    "            model = torch.load(mod)\n",
    "            model.eval()\n",
    "\n",
    "            output, individual_outputs = model(test_x.float())\n",
    "            \n",
    "            f_hats[i,:N_test] = output.detach().numpy()\n",
    "\n",
    "    return f_hats\n",
    "\n",
    "def get_predictions(f_hats):\n",
    "    f_bar = np.mean(f_hats, axis=0)\n",
    "    variances = np.mean(np.array([(f_bar - f_hat)**2 for f_hat in f_hats]), axis=0)\n",
    "\n",
    "    return f_bar, variances\n",
    "\n",
    "naive_f_hats = rep_inference(naive_models, naivetestloader)\n",
    "mimo_f_hats = rep_inference(mimo_models, testloader)\n",
    "\n",
    "naive_predcitons, naive_variances = get_predictions(naive_f_hats)\n",
    "mimo_predictions, mimo_variances = get_predictions(mimo_f_hats)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
