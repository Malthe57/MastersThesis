{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModuleWrapper for Bayesian neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for nn.Module with support for arbitrary flags and a universal forward pass\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ModuleWrapper, self).__init__()\n",
    "\n",
    "    def set_flag(self, flag_name, value):\n",
    "        setattr(self, flag_name, value)\n",
    "        for m in self.children():\n",
    "            if hasattr(m, 'set_flag'):\n",
    "                m.set_flag(flag_name, value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.children():\n",
    "            x = module(x)\n",
    "\n",
    "        kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, 'kl_loss'):\n",
    "                kl = kl + module.kl_loss()\n",
    "\n",
    "        return x, kl\n",
    "    \n",
    "class FlattenLayer(ModuleWrapper):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, self.num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_DIV(mu_q, sig_q, mu_p, sig_p):\n",
    "    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go again with the linear and convolutional layers from Kumar Shridkar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLinear(ModuleWrapper):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, priors=None):\n",
    "        super(BBBLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if priors is None:\n",
    "                priors = {\n",
    "                'prior_mu': 0,\n",
    "                'prior_sigma': 3.162,\n",
    "                'posterior_mu_initial': (0, 0.1),\n",
    "                'posterior_rho_initial': (-3, 0.1),\n",
    "            }\n",
    "        self.prior_mu = priors['prior_mu']\n",
    "        self.prior_sigma = priors['prior_sigma']\n",
    "        self.posterior_mu_initial = priors['posterior_mu_initial']\n",
    "        self.posterior_rho_initial = priors['posterior_rho_initial']\n",
    "\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.W_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "            self.bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W_mu.data.normal_(*self.posterior_mu_initial)\n",
    "        self.W_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n",
    "            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n",
    "        if self.use_bias:\n",
    "            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "            bias_var = self.bias_sigma ** 2\n",
    "        else:\n",
    "            self.bias_sigma = bias_var = None\n",
    "\n",
    "        act_mu = F.linear(x, self.W_mu, self.bias_mu)\n",
    "        act_var = 1e-16 + F.linear(x ** 2, self.W_sigma ** 2, bias_var)\n",
    "        act_std = torch.sqrt(act_var)\n",
    "\n",
    "        if self.training or sample:\n",
    "            eps = torch.empty(act_mu.size()).normal_(0, 1).to(self.device)\n",
    "            return act_mu + act_std * eps\n",
    "        else:\n",
    "            return act_mu\n",
    "\n",
    "    def kl_loss(self):\n",
    "        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n",
    "        if self.use_bias:\n",
    "            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBConv2d(ModuleWrapper):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, bias=True, priors=None):\n",
    "        super(BBBConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = 1\n",
    "        self.use_bias = bias\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if priors is None:\n",
    "            priors = {\n",
    "                'prior_mu': 0,\n",
    "                'prior_sigma': 3.162,\n",
    "                'posterior_mu_initial': (0, 0.1),\n",
    "                'posterior_rho_initial': (-3, 0.1),\n",
    "            }\n",
    "        self.prior_mu = priors['prior_mu']\n",
    "        self.prior_sigma = priors['prior_sigma']\n",
    "        self.posterior_mu_initial = priors['posterior_mu_initial']\n",
    "        self.posterior_rho_initial = priors['posterior_rho_initial']\n",
    "\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))\n",
    "        self.W_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))\n",
    "        if self.use_bias:\n",
    "            self.bias_mu = nn.Parameter(torch.Tensor(out_channels))\n",
    "            self.bias_rho = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W_mu.data.normal_(*self.posterior_mu_initial)\n",
    "        self.W_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n",
    "            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n",
    "        if self.use_bias:\n",
    "            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "            bias_var = self.bias_sigma ** 2\n",
    "        else:\n",
    "            self.bias_sigma = bias_var = None\n",
    "\n",
    "        act_mu = F.conv2d(\n",
    "            x, self.W_mu, self.bias_mu, self.stride, self.padding, self.dilation, self.groups)\n",
    "        act_var = 1e-16 + F.conv2d(\n",
    "            x ** 2, self.W_sigma ** 2, bias_var, self.stride, self.padding, self.dilation, self.groups)\n",
    "        act_std = torch.sqrt(act_var)\n",
    "\n",
    "        if self.training or sample:\n",
    "            eps = torch.empty(act_mu.size()).normal_(0, 1).to(self.device)\n",
    "            return act_mu + act_std * eps\n",
    "        else:\n",
    "            return act_mu\n",
    "\n",
    "    def kl_loss(self):\n",
    "        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n",
    "        if self.use_bias:\n",
    "            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 *5) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.120\n",
      "[1,   400] loss: 1.947\n",
      "[1,   600] loss: 1.834\n",
      "[2,   200] loss: 1.765\n",
      "[2,   400] loss: 1.741\n",
      "[2,   600] loss: 1.725\n",
      "[3,   200] loss: 1.717\n",
      "[3,   400] loss: 1.691\n",
      "[3,   600] loss: 1.684\n",
      "[4,   200] loss: 1.688\n",
      "[4,   400] loss: 1.668\n",
      "[4,   600] loss: 1.672\n",
      "[5,   200] loss: 1.675\n",
      "[5,   400] loss: 1.652\n",
      "[5,   600] loss: 1.680\n",
      "[6,   200] loss: 1.673\n",
      "[6,   400] loss: 1.654\n",
      "[6,   600] loss: 1.650\n",
      "[7,   200] loss: 1.660\n",
      "[7,   400] loss: 1.654\n",
      "[7,   600] loss: 1.648\n",
      "[8,   200] loss: 1.642\n",
      "[8,   400] loss: 1.645\n",
      "[8,   600] loss: 1.660\n",
      "[9,   200] loss: 1.651\n",
      "[9,   400] loss: 1.643\n",
      "[9,   600] loss: 1.639\n",
      "[10,   200] loss: 1.639\n",
      "[10,   400] loss: 1.648\n",
      "[10,   600] loss: 1.644\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it all over but with the Bayesian network. We first define the Bayesian network as described by Kumar Shrdikar et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(ModuleWrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = BBBConv2d(3, 6, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = BBBConv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = FlattenLayer(16*5*5)\n",
    "        self.fc1 = BBBLinear(16 * 5 * 5, 120)\n",
    "        self.fc2 = BBBLinear(120, 84)\n",
    "        self.fc3 = BBBLinear(84, 10)\n",
    "\n",
    "        self.num_classes = 10\n",
    "\n",
    "bayes_net = BayesNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in net: 62006\n",
      "Number of parameters in bayes_net: 124012\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Number of parameters in net: {count_parameters(net)}')\n",
    "print(f'Number of parameters in bayes_net: {count_parameters(bayes_net)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some stuff for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(batch_idx, m, beta_type, epoch, num_epochs):\n",
    "    if type(beta_type) is float:\n",
    "        return beta_type\n",
    "\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "    elif beta_type == \"Soenderby\":\n",
    "        if epoch is None or num_epochs is None:\n",
    "            raise ValueError('Soenderby method requires both epoch and num_epochs to be passed.')\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\":\n",
    "        beta = 1 / m\n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta\n",
    "\n",
    "def acc(outputs, targets):\n",
    "    return np.mean(outputs.cpu().numpy().argmax(axis=1) == targets.data.cpu().numpy())\n",
    "\n",
    "def logmeanexp(x, dim=None, keepdim=False):\n",
    "    \"\"\"Stable computation of log(mean(exp(x))\"\"\"\n",
    "\n",
    "    \n",
    "    if dim is None:\n",
    "        x, dim = x.view(-1), 0\n",
    "    x_max, _ = torch.max(x, dim, keepdim=True)\n",
    "    x = x_max + torch.log(torch.mean(torch.exp(x - x_max), dim, keepdim=True))\n",
    "    return x if keepdim else x.squeeze(dim)\n",
    "\n",
    "class ELBO(nn.Module):\n",
    "    def __init__(self, train_size):\n",
    "        super(ELBO, self).__init__()\n",
    "        self.train_size = train_size\n",
    "\n",
    "    def forward(self, input, target, kl, beta):\n",
    "        assert not target.requires_grad\n",
    "        return F.nll_loss(input, target, reduction='mean') * self.train_size + beta * kl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(net, optimizer, criterion, trainloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
    "    net.train()\n",
    "    training_loss = 0.0\n",
    "    accs = []\n",
    "    kl_list = []\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 1):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "\n",
    "        kl = 0.0\n",
    "        for j in range(num_ens):\n",
    "            net_out, _kl = net(inputs)\n",
    "            kl += _kl\n",
    "            outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
    "        \n",
    "        kl = kl / num_ens\n",
    "        kl_list.append(kl.item())\n",
    "        log_outputs = logmeanexp(outputs, dim=2)\n",
    "    \n",
    "        beta = get_beta(i-1, len(trainloader), beta_type, epoch, num_epochs)\n",
    "        loss = criterion(log_outputs, labels, kl, beta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accs.append(acc(log_outputs.data, labels))\n",
    "        training_loss += loss.cpu().data.numpy()\n",
    "    \n",
    "    return training_loss/len(trainloader), np.mean(accs), np.mean(kl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training Loss: 258144.8197480419\n",
      "Training Accuracy: 0.38696851023017903\n",
      "Training KL: 131173791.69309463\n",
      "Epoch 2/5\n",
      "Training Loss: 242784.66390265344\n",
      "Training Accuracy: 0.4712675831202046\n",
      "Training KL: 123194803.49872123\n",
      "Epoch 3/5\n",
      "Training Loss: 228202.37667838874\n",
      "Training Accuracy: 0.5043757992327366\n",
      "Training KL: 114941333.2173913\n",
      "Epoch 4/5\n",
      "Training Loss: 215516.24528952205\n",
      "Training Accuracy: 0.518781969309463\n",
      "Training KL: 107103090.07672635\n",
      "Epoch 5/5\n",
      "Training Loss: 203777.07822989929\n",
      "Training Accuracy: 0.5347066815856778\n",
      "Training KL: 99804608.17391305\n"
     ]
    }
   ],
   "source": [
    "criterion = ELBO(len(trainset)).to(device)\n",
    "optimizer = optim.Adam(bayes_net.parameters(), lr=0.001)\n",
    "train_ens = 16 # change back to 1\n",
    "beta_type = 'Blundell' \n",
    "n_epochs = 5\n",
    "bayes_net.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc, train_kl = train_model(bayes_net, optimizer, criterion, trainloader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    print(f'Training Loss: {train_loss}')\n",
    "    print(f'Training Accuracy: {train_acc}')\n",
    "    print(f'Training KL: {train_kl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to evaluate the Bayesian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n",
    "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
    "    net.eval()\n",
    "    valid_loss = 0.0\n",
    "    accs = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(validloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
    "        kl = 0.0\n",
    "        for j in range(num_ens):\n",
    "            net_out, _kl = net(inputs)\n",
    "            kl += _kl\n",
    "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
    "\n",
    "        log_outputs = logmeanexp(outputs, dim=2)\n",
    "\n",
    "        beta = get_beta(i-1, len(validloader), beta_type, epoch, num_epochs)\n",
    "        valid_loss += criterion(log_outputs, labels, kl, beta).item()\n",
    "        accs.append(acc(log_outputs, labels))\n",
    "\n",
    "    return valid_loss/len(validloader), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 samples\n",
      "646234.1287818472\n",
      "0.4987062101910828\n",
      "Using 2 samples\n",
      "1216324.0029856688\n",
      "0.5276671974522293\n",
      "Using 4 samples\n",
      "2362089.5488903266\n",
      "0.5534434713375797\n",
      "Using 8 samples\n",
      "4658798.857583599\n",
      "0.5676751592356688\n",
      "Using 16 samples\n",
      "9252940.180931529\n",
      "0.5747412420382165\n",
      "Using 32 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000022B0668BE20>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1437, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18442346.91503284\n",
      "0.578125\n"
     ]
    }
   ],
   "source": [
    "criterion = ELBO(len(trainset)).to(device)\n",
    "val_ensembles = [1,2,4,8,16,32]\n",
    "beta_type = 'Blundell'\n",
    "n_epochs = 1\n",
    "bayes_net.to(device)\n",
    "for val_ens in val_ensembles:\n",
    "    print(f\"Using {val_ens} samples\")\n",
    "    for epoch in range(n_epochs):\n",
    "        val_loss, val_acc = validate_model(bayes_net, criterion, testloader, num_ens=val_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n",
    "        print(val_loss)\n",
    "        print(val_acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let us try to experiment with Rank-1 VI Bayesian neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
