{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes-by-backprop prototype**\n",
    "\n",
    "In this notebook, we implement a small prototype for Bayes-by-backprop introduced by Blundell et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(1871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data function\n",
    "f = lambda x, epsilon: x + 0.3 * np.sin(2*np.pi * (x+epsilon)) + 0.3 * np.sin(4 * np.pi * (x+epsilon)) + epsilon\n",
    "\n",
    "def generate_data(N, lower, upper, std, f=f):\n",
    "    # create data\n",
    "    x = np.linspace(lower, upper, N)\n",
    "\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        epsilon = np.random.normal(0, std)\n",
    "        y.append(f(x[i], epsilon))\n",
    "    return x, y\n",
    "\n",
    "# Generate train data\n",
    "N_train = 2000\n",
    "x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate validation data\n",
    "N_val = 500\n",
    "x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate test data\n",
    "N_test = 500\n",
    "x_test, y_test = generate_data(N_test, lower=-0.5, upper=1.5, std=0.02)\n",
    "\n",
    "line = f(x_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    return torch.tensor(x)[:,None].T, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a diagonal Gaussian distribution (zero covariance) as the variational posterior. Rather than using just $\\mu$ and $\\sigma$ as the variational parameters, the standard deviation is parameterised as:\n",
    "$$\n",
    "\\sigma = \\log{(1 + \\exp{(\\rho)})}\n",
    "$$\n",
    "such that $\\sigma$ is always non-negative. The variational parameters are then $\\mathbf{\\theta} = (\\mu, \\rho)$. \n",
    "\n",
    "The code blocks in the following sections are inspired by:\n",
    "\n",
    "https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property # @property so we can call self.sigma directly rather than self.sigma()\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def sample(self):  \n",
    "        # sample noise from normal distribution \n",
    "        epsilon = self.normal.sample(self.rho.size())\n",
    "        return self.mu + self.sigma * epsilon # scale with mu and sigma\n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        # log pdf for Gaussian distribution\n",
    "        return torch.sum(-torch.log(self.sigma) - 0.5*np.log(2*np.pi) - 0.5 * ((w - self.mu) / self.sigma)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Gaussian`` class is a simple class that allows us to sample from a Gaussian distribution, representing the variational posterior.\n",
    "\n",
    "The function ``sigma`` computes the standard deviation $\\sigma$ for a given $\\rho$ value.\n",
    "The function ``sample`` allows us to sample from the approximate posterior, using the reparametrisation trick with $\\mu$ and $\\sigma$. \n",
    "The function ``log_prob`` computes the log-probability density function for a normal distribution wtih mean $\\mu$ and standard devation $\\sigma$ (derivation below):\n",
    "\n",
    "The probability density function for the weights $\\mathbf{w}$ given the variational parameters $\\mathbf{\\theta} = (\\mu, \\rho)$ of a Gaussian distribution is given as\n",
    "\\begin{align*}\n",
    "q(\\mathbf{w|\\mathbf{\\theta}}) &= \\prod_j \\mathcal{N}(w_j | \\mu, \\sigma) \\\\\n",
    "& = \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2}\n",
    "\\end{align*}\n",
    "Then taking the log, we get:\n",
    "\\begin{align*}\n",
    "\\log{q(\\mathbf{w}|\\mathbf{\\theta})} &=  \\log{  \\left( \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2} \\right)} \\\\\n",
    "           &= \\sum_j \\log{(1)} - \\log{\\left(\\sigma \\sqrt(2 \\pi)\\right) - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2 } \\\\\n",
    "           &= \\sum_j -\\log{(\\sigma)} - \\frac12 \\log{(2\\pi)} - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior that was proposed in the paper by Blundell et al. is a Gaussian mixture prior over the weights $\\mathbf{w}$. \n",
    "$$\n",
    "P(\\mathbf{w}) = \\prod_j \\pi \\mathcal{N}(w_j|0, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(w_j| 0, \\sigma_2^2) \n",
    "$$\n",
    "where $\\pi \\in [0,1]$ is the mixture weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMixturePrior(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Implementing the scale mixture prior in equation 7 of the paper.\n",
    "        From the paper: sigma1 > sigma2 and sigma2 << 1.\n",
    "        \"\"\"\n",
    "        assert sigma1 > sigma2, \"Error: sigma1 must be greater than sigma2.\"\n",
    "        assert sigma2 < 1, \"Error: sigma2 must be less than 1.\"\n",
    "\n",
    "        self.pi = pi\n",
    "        \n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        \"\"\"\n",
    "        Implementing the log pdf for the scale mixture prior\n",
    "        \"\"\"\n",
    "\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(w)) # torch.exp of log pdf so we get the pdf\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(w))\n",
    "        return torch.log(self.pi * p1 + (1-self.pi) * p2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Bayesian linear layer for our Bayesian neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by\n",
    "# https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "# https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n",
    "\n",
    "\n",
    "class BayesianLinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6]))):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-2, -1)) \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(output_dim).uniform_(-2, -1))\n",
    "\n",
    "        # create approximate posterior distribution\n",
    "        self.weight_posterior = Gaussian(self.weight_mus, self.weight_rhos)\n",
    "        self.bias_posterior = Gaussian(self.bias_mus, self.bias_rhos)\n",
    "\n",
    "        # scale mixture posterior\n",
    "        self.weight_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "        self.bias_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "\n",
    "        self.log_prior = 0.0\n",
    "        self.log_variational_posterior = 0.0\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "        if test:\n",
    "            # during inference, we simply use the mus of the weights and biases\n",
    "            w = self.weight_mus\n",
    "            b = self.bias_mus\n",
    "\n",
    "            self.log_prior = 0.0\n",
    "            self.log_variational_posterior = 0.0\n",
    "        else:\n",
    "            # sample from approximate posterior distribution\n",
    "            w = self.weight_posterior.sample()\n",
    "            b = self.bias_posterior.sample()\n",
    "\n",
    "            # compute log prior and log variational posterior\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "            # print(\"log_prior:\", self.log_prior)\n",
    "        \n",
    "        return torch.mm(x, w) + b # matrix multiply input by weights and add bias\n",
    "\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_units1, hidden_units2, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6]))):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(BayesianLinearLayer(1, hidden_units1),\n",
    "                                   nn.ReLU(),\n",
    "                                   BayesianLinearLayer(hidden_units1, hidden_units2),\n",
    "                                   nn.ReLU(),\n",
    "                                   BayesianLinearLayer(hidden_units2, 2)) # output channel = 2 for mean and variance\n",
    "\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def compute_log_prior(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                self.log_prior += layer.log_prior.sum()\n",
    "\n",
    "    def compute_log_variational_posterior(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                # print(layer.log_variational_posterior)\n",
    "                self.log_variational_posterior += layer.log_variational_posterior.sum()\n",
    "\n",
    "    def get_sigma(self, rho):\n",
    "        return torch.log1p(torch.exp(rho))\n",
    "\n",
    "\n",
    "    def compute_ELBO(self, input, target):\n",
    "        # formula from Blundell: loss = log_variational_posterior - log_prior - log_likelihood\n",
    "        #                        loss = log_variational_posterior - log_prior + NLL\n",
    "        \n",
    "        # compute log prior and log variational posterio\n",
    "\n",
    "        output = self.forward(input, test=False)\n",
    "        mu = output[:,0]\n",
    "        rho = output[:,1]\n",
    "\n",
    "        self.compute_log_prior()\n",
    "        self.compute_log_variational_posterior()\n",
    "        NLL = self.NLL_loss(target, mu, rho)\n",
    "        # print(self.log_variational_posterior)\n",
    "        # print(self.log_prior)\n",
    "        # print(NLL)\n",
    "\n",
    "        return torch.sum(self.log_variational_posterior - self.log_prior + NLL)\n",
    "\n",
    "    def NLL_loss(self, target, mu, rho):\n",
    "        # negative log likelihood loss\n",
    "        sigma = self.get_sigma(rho)\n",
    "\n",
    "        return 1 / (2 * sigma.pow(2)) * (target - mu).pow(2) + torch.log(sigma)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "traindata = ToyDataset(x, y)\n",
    "trainloader = DataLoader(traindata, batch_size=60, shuffle=True, worker_init_fn=seed_worker, generator=g, collate_fn=collate_fn)\n",
    "\n",
    "valdata = ToyDataset(x_val, y_val)\n",
    "valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "testdata = ToyDataset(x_test, y_test)\n",
    "testloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# create model\n",
    "BNN_model = BayesianNeuralNetwork(32, 128)\n",
    "optimizer = torch.optim.Adam(BNN_model.parameters(), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions 🤖\n",
    "\n",
    "def train(model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10):\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_,y_ = x_.float().T, y_.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model.compute_ELBO(x_, y_)\n",
    "            # print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())  \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float(), val_y.float()\n",
    "                \n",
    "                    val_loss = model.compute_ELBO(val_x, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12950716149a428eb3bc1b10e41fabca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBNN_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plot_loss(losses, val_losses)\n",
      "Cell \u001b[1;32mIn[172], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, trainloader, valloader, epochs, model_name, val_every_n_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_ELBO(x_, y_)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())  \n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "losses, val_losses = train(BNN_model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10)\n",
    "\n",
    "plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
