{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes-by-backprop prototype**\n",
    "\n",
    "In this notebook, we implement a small prototype for Bayes-by-backprop introduced by Blundell et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(1871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data function\n",
    "f = lambda x, epsilon: x + 0.3 * np.sin(2*np.pi * (x+epsilon)) + 0.3 * np.sin(4 * np.pi * (x+epsilon)) + epsilon\n",
    "\n",
    "def generate_data(N, lower, upper, std, f=f):\n",
    "    # create data\n",
    "    x = np.linspace(lower, upper, N)\n",
    "\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        epsilon = np.random.normal(0, std)\n",
    "        y.append(f(x[i], epsilon))\n",
    "    return x, y\n",
    "\n",
    "# Generate train data\n",
    "N_train = 2000\n",
    "x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate validation data\n",
    "N_val = 500\n",
    "x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate test data\n",
    "N_test = 500\n",
    "x_test, y_test = generate_data(N_test, lower=-0.5, upper=1.5, std=0.02)\n",
    "\n",
    "line = f(x_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    return torch.tensor(x)[:,None], torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a diagonal Gaussian distribution (zero covariance) as the variational posterior. Rather than using just $\\mu$ and $\\sigma$ as the variational parameters, the standard deviation is parameterised as:\n",
    "$$\n",
    "\\sigma = \\log{(1 + \\exp{(\\rho)})}\n",
    "$$\n",
    "such that $\\sigma$ is always non-negative. The variational parameters are then $\\mathbf{\\theta} = (\\mu, \\rho)$. \n",
    "\n",
    "The code blocks in the following sections are inspired by:\n",
    "\n",
    "https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "\n",
    "    @property # @property so we can call self.sigma directly rather than self.sigma()\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def sample(self):  \n",
    "        # sample noise from normal distribution \n",
    "        epsilon = self.normal.sample(self.rho.size())\n",
    "        epsilon = epsilon.to(self.device)\n",
    "  \n",
    "        return self.mu + self.sigma * epsilon # scale with mu and sigma\n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        # log pdf for Gaussian distribution\n",
    "        return torch.sum(-torch.log(self.sigma) - 0.5*np.log(2*np.pi) - 0.5 * ((w - self.mu) / self.sigma)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = Gaussian(torch.tensor([1.0]), torch.tensor([30.0]))\n",
    "\n",
    "gauss.log_prob(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.log1p(torch.exp(torch.tensor(30.0)))\n",
    "input = torch.tensor([1,2,3])\n",
    "mu = torch.tensor([1.0])\n",
    "\n",
    "\n",
    "(-math.log(math.sqrt(2 * math.pi))\n",
    "                - torch.log(sigma)\n",
    "                - ((input - mu) ** 2) / (2 * sigma ** 2)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Gaussian`` class is a simple class that allows us to sample from a Gaussian distribution, representing the variational posterior.\n",
    "\n",
    "The function ``sigma`` computes the standard deviation $\\sigma$ for a given $\\rho$ value.\n",
    "The function ``sample`` allows us to sample from the approximate posterior, using the reparametrisation trick with $\\mu$ and $\\sigma$. \n",
    "The function ``log_prob`` computes the log-probability density function for a normal distribution wtih mean $\\mu$ and standard devation $\\sigma$ (derivation below):\n",
    "\n",
    "The probability density function for the weights $\\mathbf{w}$ given the variational parameters $\\mathbf{\\theta} = (\\mu, \\rho)$ of a Gaussian distribution is given as\n",
    "\\begin{align*}\n",
    "q(\\mathbf{w|\\mathbf{\\theta}}) &= \\prod_j \\mathcal{N}(w_j | \\mu, \\sigma) \\\\\n",
    "& = \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2}\n",
    "\\end{align*}\n",
    "Then taking the log, we get:\n",
    "\\begin{align*}\n",
    "\\log{q(\\mathbf{w}|\\mathbf{\\theta})} &=  \\log{  \\left( \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2} \\right)} \\\\\n",
    "           &= \\sum_j \\log{(1)} - \\log{\\left(\\sigma \\sqrt(2 \\pi)\\right) - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2 } \\\\\n",
    "           &= \\sum_j -\\log{(\\sigma)} - \\frac12 \\log{(2\\pi)} - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior that was proposed in the paper by Blundell et al. is a Gaussian mixture prior over the weights $\\mathbf{w}$. \n",
    "$$\n",
    "P(\\mathbf{w}) = \\prod_j \\pi \\mathcal{N}(w_j|0, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(w_j| 0, \\sigma_2^2) \n",
    "$$\n",
    "where $\\pi \\in [0,1]$ is the mixture weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMixturePrior(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Implementing the scale mixture prior in equation 7 of the paper.\n",
    "        From the paper: sigma1 > sigma2 and sigma2 << 1.\n",
    "        \"\"\"\n",
    "        assert sigma1 > sigma2, \"Error: sigma1 must be greater than sigma2.\"\n",
    "        assert sigma2 < 1, \"Error: sigma2 must be less than 1.\"\n",
    "\n",
    "        self.pi = pi\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.gaussian1 = torch.distributions.Normal(torch.tensor(0).to(self.device),sigma1.to(self.device))\n",
    "        self.gaussian2 = torch.distributions.Normal(torch.tensor(0).to(self.device),sigma2.to(self.device))\n",
    "       \n",
    "\n",
    "    def log_prob(self, w):\n",
    "        \"\"\"\n",
    "        Implementing the log pdf for the scale mixture prior\n",
    "        \"\"\"\n",
    "\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(w)) # torch.exp of log pdf so we get the pdf\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(w))\n",
    "        return torch.log(self.pi * p1 + (1-self.pi) * p2).sum()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture = ScaleMixturePrior(pi=torch.tensor(0.5), sigma1=torch.exp(torch.tensor(0)), sigma2=torch.exp(torch.tensor(-6)))\n",
    "\n",
    "mixture.log_prob(torch.tensor([1,2,3]).to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Bayesian linear layer for our Bayesian neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "traindata = ToyDataset(x, y)\n",
    "trainloader = DataLoader(traindata, batch_size=512, shuffle=True, worker_init_fn=seed_worker, generator=g, collate_fn=collate_fn)\n",
    "\n",
    "valdata = ToyDataset(x_val, y_val)\n",
    "valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "testdata = ToyDataset(x_test, y_test)\n",
    "testloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "NUM_BATCHES = len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BayesianLinearLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6]))):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.pi = pi\n",
    "#         self.sigma1 = sigma1\n",
    "#         self.sigma2 = sigma2\n",
    "\n",
    "#         # initialise mu and rho parameters so they get updated in backpropagation\n",
    "#         self.weight_mus = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.05, 0.05))\n",
    "#         self.weight_rhos = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-2, -1)) \n",
    "#         self.bias_mus = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.05, 0.05))\n",
    "#         self.bias_rhos = nn.Parameter(torch.Tensor(output_dim).uniform_(-2, -1))\n",
    "\n",
    "#         # create approximate posterior distribution\n",
    "#         self.weight_posterior = Gaussian(self.weight_mus, self.weight_rhos)\n",
    "#         self.bias_posterior = Gaussian(self.bias_mus, self.bias_rhos)\n",
    "\n",
    "#         # scale mixture posterior\n",
    "#         self.weight_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "#         self.bias_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "\n",
    "#         self.log_prior = 0.0\n",
    "#         self.log_variational_posterior = 0.0\n",
    "\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     def forward(self, x, inference=False):\n",
    "#         if inference:\n",
    "#             w = self.weight_mus\n",
    "#             b = self.bias_mus\n",
    "#             output = torch.mm(x, w) + b\n",
    "\n",
    "#             self.log_prior = 0\n",
    "#             self.log_variational_posterior = 0\n",
    "\n",
    "            \n",
    "#         else:\n",
    "#             # sample from approximate posterior distribution\n",
    "#             w = self.weight_posterior.sample()\n",
    "#             b = self.bias_posterior.sample()\n",
    "\n",
    "#             # compute log prior and log variational posterior\n",
    "#             self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "#             self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "\n",
    "#         output = torch.mm(x, w) + b\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BayesianNeuralNetwork(nn.Module):\n",
    "#     def __init__(self, hidden_units1, hidden_units2, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6])), device=\"cpu\"):\n",
    "#         super().__init__()\n",
    "#         self.hidden_units1 = hidden_units1\n",
    "#         self.hidden_units2 = hidden_units2\n",
    "#         self.pi = pi\n",
    "#         self.sigma1 = sigma1\n",
    "#         self.sigma2 = sigma2\n",
    "\n",
    "#         self.l1 = BayesianLinearLayer(1, hidden_units1)\n",
    "#         self.l2 = BayesianLinearLayer(hidden_units1, hidden_units2)\n",
    "#         self.l3 = BayesianLinearLayer(hidden_units2, 2)\n",
    "\n",
    "#         self.layers = [self.l1, self.l2, self.l3]\n",
    "        \n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, x, inference=False):\n",
    "#         x = x.to(self.device)\n",
    "#         x = F.relu(self.l1(x.to(self.device), inference))\n",
    "#         x = F.relu(self.l2(x.to(self.device), inference))\n",
    "#         x = self.l3(x.to(self.device), inference)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def compute_log_prior(self):\n",
    "#         log_prior = 0\n",
    "#         for layer in self.layers:\n",
    "#            log_prior += layer.log_prior\n",
    "#         return log_prior\n",
    "    \n",
    "#     def compute_log_variational_posterior(self):\n",
    "#         log_variational_posterior = 0\n",
    "#         for layer in self.layers:\n",
    "#             log_variational_posterior += layer.log_variational_posterior\n",
    "#         return log_variational_posterior\n",
    "    \n",
    "#     def get_sigma(self, rho):\n",
    "#         return torch.log1p(torch.exp(rho))\n",
    "    \n",
    "#     def log_gaussian_loss(self, target, mu, sigma):\n",
    "#         target = target.to(self.device)\n",
    "#         mu = mu.to(self.device)\n",
    "#         sigma = sigma.to(self.device)\n",
    "#         return (-0.5 * torch.log(torch.tensor(2 * np.pi)) - torch.log(sigma)) - (target - mu)**2 / (2 * sigma**2).mean().float()\n",
    "    \n",
    "#     def compute_ELBO(self, input, target, n_samples=10):\n",
    "#         outputs = torch.zeros(n_samples, target.size(0)).to(self.device)\n",
    "#         log_priors = torch.zeros(n_samples).to(self.device)\n",
    "#         log_variational_posteriors = torch.zeros(n_samples).to(self.device)\n",
    "#         NLLs = torch.zeros(n_samples).to(self.device)\n",
    "#         for i in range(n_samples):\n",
    "#             outputs = self.forward(input, inference=False)\n",
    "#             mu = outputs[:,0]\n",
    "#             sigma = self.get_sigma(outputs[:,1])\n",
    "#             # print(sigma)\n",
    "#             log_priors[i] = self.compute_log_prior()\n",
    "#             log_variational_posteriors[i] = self.compute_log_variational_posterior()\n",
    "#             NLLs[i] = -self.log_gaussian_loss(target, mu, sigma).sum()\n",
    "#         log_prior = log_priors.mean(0)\n",
    "#         log_variational_posterior = log_variational_posteriors.mean(0)\n",
    "#         NLL = NLLs.mean(0)\n",
    "\n",
    "#         loss = log_variational_posterior - log_prior + NLL\n",
    "#         return loss, log_prior, log_variational_posterior, NLL\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6]))):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-2, -1)) \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(output_dim).uniform_(-2, -1))\n",
    "\n",
    "        # create approximate posterior distribution\n",
    "        self.weight_posterior = Gaussian(self.weight_mus, self.weight_rhos)\n",
    "        self.bias_posterior = Gaussian(self.bias_mus, self.bias_rhos)\n",
    "\n",
    "        # scale mixture posterior\n",
    "        self.weight_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "        self.bias_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "\n",
    "        self.log_prior = 0.0\n",
    "        self.log_variational_posterior = 0.0\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, x, inference=False):\n",
    "        if inference:\n",
    "            # during inference, we simply use the mus of the weights and biases\n",
    "            w = self.weight_mus\n",
    "            b = self.bias_mus\n",
    "\n",
    "            self.log_prior = 0.0\n",
    "            self.log_variational_posterior = 0.0\n",
    "        else:\n",
    "            # sample from approximate posterior distribution\n",
    "            w = self.weight_posterior.sample()\n",
    "            b = self.bias_posterior.sample()\n",
    "\n",
    "            # compute log prior and log variational posterior\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "            # print(\"log_prior:\", self.log_prior)\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        return torch.mm(x, w) + b # matrix multiply input by weights and add bias\n",
    "\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_units1, hidden_units2, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6])), device='cpu'):\n",
    "        super().__init__()\n",
    "        self.l1 = BayesianLinearLayer(1, hidden_units1)\n",
    "        self.l2 = BayesianLinearLayer(hidden_units1, hidden_units2)\n",
    "        self.l3 = BayesianLinearLayer(hidden_units2, 2) # output channel = 2 for mean and variance\n",
    "\n",
    "        self.model = [self.l1, self.l2, self.l3]\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, inference=False):\n",
    "        x = x.to(self.device)\n",
    "        x = F.relu(self.l1(x.to(self.device), inference))\n",
    "        x = F.relu(self.l2(x.to(self.device), inference))\n",
    "        x = self.l3(x.to(self.device), inference)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def compute_log_prior(self):\n",
    "        # return self.model[0].log_prior + self.model[2].log_prior + self.model[4].log_prior\n",
    "        model_log_prior = 0.0\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                # print(layer.log_prior.shape)\n",
    "                model_log_prior += layer.log_prior\n",
    "\n",
    "        return model_log_prior\n",
    "\n",
    "    def compute_log_variational_posterior(self):\n",
    "        # return self.model[0].log_variational_posterior + self.model[2].log_variational_posterior + self.model[4].log_variational_posterior\n",
    "        model_log_variational_posterior = 0.0\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                # print(layer.log_variational_posterior)\n",
    "                model_log_variational_posterior += layer.log_variational_posterior\n",
    "\n",
    "        return model_log_variational_posterior\n",
    "\n",
    "    def get_sigma(self, rho):\n",
    "        return torch.log1p(torch.exp(rho))\n",
    "\n",
    "\n",
    "    def compute_ELBO(self, input, target, n_samples=10):\n",
    "        # formula from Blundell: loss = log_variational_posterior - log_prior - log_likelihood\n",
    "        #                        loss = log_variational_posterior - log_prior + NLL\n",
    "        \n",
    "        # compute log prior and log variational posterio\n",
    "\n",
    "        mus  = torch.zeros(n_samples, input.shape[0])\n",
    "        rhos = torch.zeros(n_samples, input.shape[0])       \n",
    "        log_priors = torch.zeros(n_samples)\n",
    "        log_variational_posteriors = torch.zeros(n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            output = self.forward(input, inference=False).to(self.device)\n",
    "            mu = output[:,0]\n",
    "            rho = output[:,1]\n",
    "\n",
    "            log_priors[i] = self.compute_log_prior()\n",
    "            log_variational_posteriors[i] = self.compute_log_variational_posterior()\n",
    "            mus[i] = mu\n",
    "            rhos[i] = rho\n",
    "\n",
    "\n",
    "        # self.NLL = self.NLL_loss(target, mus, rhos)\n",
    "        # print(\"target\", target.shape)\n",
    "        # print(\"mus\", mus.shape)\n",
    "        # print(\"sigmas\", self.get_sigma(rhos).shape)\n",
    "\n",
    "        \n",
    "\n",
    "        self.NLL = torch.nn.GaussianNLLLoss(reduction='mean')(target, mus.mean(0).to(self.device), self.get_sigma(rhos).mean(0).to(self.device))\n",
    "\n",
    "        loss = (log_variational_posteriors.mean() - log_priors.mean()) / NUM_BATCHES + self.NLL\n",
    "\n",
    "        # print(log_priors.mean(), log_variational_posteriors.mean(), self.NLL, loss)\n",
    "\n",
    "        return loss, log_priors.mean(), log_variational_posteriors.mean(), self.NLL\n",
    "\n",
    "    def NLL_loss(self, target, mu, rho):\n",
    "        # negative log likelihood loss\n",
    "\n",
    "        sigma = self.get_sigma(rho)\n",
    "        \n",
    "        # take mean over samples\n",
    "        mu = mu.mean(dim=0)\n",
    "        sigma = sigma.mean(dim=0)\n",
    "\n",
    "        mu = mu.to(self.device)\n",
    "        sigma = sigma.to(self.device)\n",
    "\n",
    "        # mean reduction \n",
    "        # print((1 / (2 * sigma.pow(2)) * (target - mu).pow(2) + torch.log(sigma)).mean())\n",
    "        norm = torch.distributions.Normal(mu, sigma)\n",
    "        # print(norm.log_prob(target).mean())\n",
    "\n",
    "        # return (1 / (2 * sigma.pow(2)) * (target - mu).pow(2) + torch.log(sigma)).mean()\n",
    "        return -norm.log_prob(target).mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create model\n",
    "BNN_model = BayesianNeuralNetwork(32, 128, device=device)\n",
    "BNN_model.to(device)\n",
    "optimizer = torch.optim.SGD(BNN_model.parameters(), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1.0]).to(device)\n",
    "mu = torch.tensor([1.0]).to(device)\n",
    "rho = torch.tensor([-6.1]).to(device)\n",
    "\n",
    "BNN_model.NLL_loss(target, mu, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_test, y_test in testloader:\n",
    "    x_test = x_test.float().to(device)\n",
    "    y_test = y_test.float().to(device)\n",
    "\n",
    "    loss, log_prior, log_variational_posterior, NLL = BNN_model.compute_ELBO(x_test, y_test, n_samples=10)\n",
    "    print(\"ELBO:\", loss)\n",
    "    print(\"log_prior:\", log_prior)\n",
    "    print(\"log_variational_posterior:\", log_variational_posterior)\n",
    "    print(\"NLL:\", NLL)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_test, y_test in testloader:\n",
    "    x_test = x_test.float().to(device)\n",
    "    y_test = y_test.float().to(device)\n",
    "\n",
    "    output = BNN_model(x_test, inference=True)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = output[:,0]\n",
    "print(mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions ðŸ¤–\n",
    "\n",
    "def train_BNN(model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10, device='cpu'):\n",
    "\n",
    "    losses = []\n",
    "    log_priors = []\n",
    "    log_variational_posteriors = []\n",
    "    NLLs = []\n",
    "\n",
    "    val_losses = []\n",
    "    val_log_priors = []\n",
    "    val_log_variational_posteriors = []\n",
    "    val_NLLs = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_, y_ = x_.float().to(device), y_.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, log_prior, log_posterior, log_NLL = model.compute_ELBO(x_, y_)\n",
    "            # print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item()) \n",
    "            log_priors.append(log_prior.item())\n",
    "            log_variational_posteriors.append(log_posterior.item())\n",
    "            NLLs.append(log_NLL.item()) \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float().to(device), val_y.float().to(device)\n",
    "                \n",
    "                    val_loss, _ , _, _ = model.compute_ELBO(val_x, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, log_priors, log_variational_posteriors, NLLs, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_log_probs(log_priors, log_variational_posteriors, NLLs):\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "\n",
    "    ax[0].plot(log_priors, label='Train log prior')\n",
    "    ax[0].set_title('Train log prior')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Log prior')\n",
    "\n",
    "    ax[1].plot(log_variational_posteriors, label='Train log variational posterior', color='orange')\n",
    "    ax[1].set_title('Train log variational posterior')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Log variational posterior')\n",
    "\n",
    "    ax[2].plot(NLLs, label='Train NLL', color='green')\n",
    "    ax[2].set_title('Train NLL')\n",
    "    ax[2].set_xlabel('Iterations')\n",
    "    ax[2].set_ylabel('NLL')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, log_priors, log_variational_posteriors, NLLs, val_losses = train_BNN(BNN_model, optimizer, trainloader, valloader, epochs=2500, model_name='BNN', val_every_n_epochs=10, device=device)\n",
    "\n",
    "plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_probs(log_priors, log_variational_posteriors, NLLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We perform inference with the BNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"BNN.pt\")\n",
    "\n",
    "predictions = []\n",
    "stdds = []\n",
    "\n",
    "for x_test, y_test in testloader:\n",
    "    x_test, y_test = x_test.float(), y_test.float()\n",
    "    with torch.no_grad():\n",
    "        output = model(x_test, test=False)\n",
    "        mu = output[:,0]\n",
    "        rho = output[:,1]\n",
    "        sigma = model.get_sigma(rho)\n",
    "        predictions.append(mu.cpu().detach().numpy())\n",
    "        stdds.append(sigma.cpu().detach().numpy())\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1,1, figsize=(18,12))\n",
    "\n",
    "### plot mimo ###\n",
    "plt.plot(x, y, '.', label='Train data', color='orange')\n",
    "plt.plot(x_test[:,0], line, '--', label='true function', color='red')\n",
    "# plot test data\n",
    "plt.plot(x_test[:,0], y_test, '.', label='Test data', color='black')\n",
    "\n",
    "# plot predicitons with confidence intervals\n",
    "plt.plot(x_test[:,0], predictions[0], '-', label=f'Mean MIMO Predictions', linewidth=2)\n",
    "plt.fill_between(x_test[:,0], predictions[0] - 1.96*stdds[0], predictions[0] + 1.96*stdds[0], alpha=0.2, label=f'Confidence Interval')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
