{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes-by-backprop prototype**\n",
    "\n",
    "In this notebook, we implement a small prototype for Bayes-by-backprop introduced by Blundell et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(1871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data function\n",
    "f = lambda x, epsilon: x + 0.3 * np.sin(2*np.pi * (x+epsilon)) + 0.3 * np.sin(4 * np.pi * (x+epsilon)) + epsilon\n",
    "\n",
    "def generate_data(N, lower, upper, std, f=f):\n",
    "    # create data\n",
    "    x = np.linspace(lower, upper, N)\n",
    "\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        epsilon = np.random.normal(0, std)\n",
    "        y.append(f(x[i], epsilon))\n",
    "    return x, y\n",
    "\n",
    "# Generate train data\n",
    "N_train = 2000\n",
    "x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate validation data\n",
    "N_val = 500\n",
    "x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate test data\n",
    "N_test = 500\n",
    "x_test, y_test = generate_data(N_test, lower=-0.5, upper=1.5, std=0.02)\n",
    "\n",
    "line = f(x_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def train_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for training MIMO\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "    \n",
    "    x_chunks = torch.stack(torch.chunk(torch.tensor(x), M, dim=0), dim=1)\n",
    "    y_chunks = torch.stack(torch.chunk(torch.tensor(y), M, dim=0), dim=1)\n",
    "\n",
    "    return x_chunks, y_chunks\n",
    "\n",
    "def test_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for testing MIMO\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "    x = torch.tensor(x)[:,None].repeat(1,M)\n",
    "    y = torch.tensor(y)[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def naive_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for naive multiheaded model\"\"\"\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "    x = torch.tensor(x)[:,None]\n",
    "    y = torch.tensor(y)[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a diagonal Gaussian distribution (zero covariance) as the variational posterior. Rather than using just $\\mu$ and $\\sigma$ as the variational parameters, the standard deviation is parameterised as:\n",
    "$$\n",
    "\\sigma = \\log{(1 + \\exp{(\\rho)})}\n",
    "$$\n",
    "such that $\\sigma$ is always non-negative. The variational parameters are then $\\mathbf{\\theta} = (\\mu, \\rho)$. \n",
    "\n",
    "The code blocks in the following sections are inspired by:\n",
    "\n",
    "https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property # @property so we can call self.sigma directly rather than self.sigma()\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def sample(self):  \n",
    "        # sample noise from normal distribution \n",
    "        epsilon = self.normal.sample(self.rho.size())\n",
    "        return self.mu + self.sigma * epsilon # scale with mu and sigma\n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        # log pdf for Gaussian distribution\n",
    "        return torch.sum(-torch.log(self.sigma) - 0.5*np.log(2*np.pi) - 0.5 * ((w - self.mu) / self.sigma)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Gaussian`` class is a simple class that allows us to sample from a Gaussian distribution, representing the variational posterior.\n",
    "\n",
    "The function ``sigma`` computes the standard deviation $\\sigma$ for a given $\\rho$ value.\n",
    "The function ``sample`` allows us to sample from the approximate posterior, using the reparametrisation trick with $\\mu$ and $\\sigma$. \n",
    "The function ``log_prob`` computes the log-probability density function for a normal distribution wtih mean $\\mu$ and standard devation $\\sigma$ (derivation below):\n",
    "\n",
    "The probability density function for the weights $\\mathbf{w}$ given the variational parameters $\\mathbf{\\theta} = (\\mu, \\rho)$ of a Gaussian distribution is given as\n",
    "\\begin{align*}\n",
    "q(\\mathbf{w|\\mathbf{\\theta}}) &= \\prod_j \\mathcal{N}(w_j | \\mu, \\sigma) \\\\\n",
    "& = \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2}\n",
    "\\end{align*}\n",
    "Then taking the log, we get:\n",
    "\\begin{align*}\n",
    "\\log{q(\\mathbf{w}|\\mathbf{\\theta})} &=  \\log{  \\left( \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2} \\right)} \\\\\n",
    "           &= \\sum_j \\log{(1)} - \\log{\\left(\\sigma \\sqrt(2 \\pi)\\right) - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2 } \\\\\n",
    "           &= \\sum_j -\\log{(\\sigma)} - \\frac12 \\log{(2\\pi)} - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior that was proposed in the paper by Blundell et al. is a Gaussian mixture prior over the weights $\\mathbf{w}$. \n",
    "$$\n",
    "P(\\mathbf{w}) = \\prod_j \\pi \\mathcal{N}(w_j|0, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(w_j| 0, \\sigma_2^2) \n",
    "$$\n",
    "where $\\pi \\in [0,1]$ is the mixture weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMixturePrior(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Implementing the scale mixture prior in equation 7 of the paper.\n",
    "        From the paper: sigma1 > sigma2 and sigma2 << 1.\n",
    "        \"\"\"\n",
    "        assert sigma1 > sigma2, \"Error: sigma1 must be greater than sigma2.\"\n",
    "        assert sigma2 < 1, \"Error: sigma2 must be less than 1.\"\n",
    "\n",
    "        self.pi = pi\n",
    "        \n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.gaussian1 = torch.distributions.Normal(0,sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,sigma2)\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        \"\"\"\n",
    "        Implementing the log pdf for the scale mixture prior\n",
    "        \"\"\"\n",
    "\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(w)) # torch.exp of log pdf so we get the pdf\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(w))\n",
    "        return torch.log(self.pi * p1 + (1-self.pi) * p2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Bayesian linear layer for our Bayesian neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by\n",
    "# https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "# https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class BayesianLinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pi=0.5, sigma1=torch.exp(torch.tensor([-0])), sigma2=torch.exp(torch.tensor([-6]))):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-2, -1)) \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(output_dim).uniform_(-2, -1))\n",
    "\n",
    "        # create approximate posterior distribution\n",
    "        self.weight_posterior = Gaussian(self.weight_mus, self.weight_rhos)\n",
    "        self.bias_posterior = Gaussian(self.bias_mus, self.bias_rhos)\n",
    "\n",
    "        # scale mixture posterior\n",
    "        self.weight_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "        self.bias_prior = ScaleMixturePrior(pi=pi, sigma1=sigma1, sigma2=sigma2)\n",
    "\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "        if test:\n",
    "            # during inference, we simply use the mu and sigma\n",
    "            w = self.weight_mus\n",
    "            b = self.bias_mus\n",
    "\n",
    "            self.log_prior = 0\n",
    "            self.log_variational_posterior = 0\n",
    "        else:\n",
    "            # sample from approximate posterior distribution\n",
    "            w = self.weight_posterior.sample()\n",
    "            b = self.bias_posterior.sample()\n",
    "\n",
    "            # compute KL loss\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "\n",
    "        return torch.mm(x, w) + b # matrix multiply input by weights and add bias\n",
    "\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_units1, hidden_units2, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(BayesianLinearLayer(1, hidden_units1),\n",
    "                                   nn.ReLU(),\n",
    "                                   BayesianLinearLayer(hidden_units1, hidden_units2),\n",
    "                                   nn.ReLU(),\n",
    "                                   BayesianLinearLayer(hidden_units2, 2)) # output channel = 2 for mean and variance\n",
    "\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x, test=False):\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def log_prior(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                self.log_prior += layer.log_prior \n",
    "\n",
    "    def log_variational_posterior(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                self.log_variational_posterior += layer.log_variational_posterior\n",
    "\n",
    "    def compute_KL_divergence(self):\n",
    "\n",
    "        self.log_prior()\n",
    "        self.log_variational_posterior()\n",
    "\n",
    "        return self.log_variational_posterior - self.log_prior\n",
    "\n",
    "    # use conditional Gaussian loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
