{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MIMO PROTOYPE**:\n",
    "\n",
    "Here we try our hand at making MIMO work, and testing its validity with a synthetic toy dataset. The notebook contains the following: \n",
    "- Create toy dataset and dataloaders - toy regression data & MNIST?\n",
    "- Simple MIMO model - regression and classification\n",
    "- Training and results on toy dataset\n",
    "\n",
    "For the classification task, we make the following plots:\n",
    "- Reliability: confidence vs error - for classification\n",
    "- Expected calibration errors - for classification\n",
    "- Diveristity plots - for classification and regression\n",
    "\n",
    "For the regression task, we plot the mean prediction and a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(1871)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Making the dataset***\n",
    "\n",
    "We start by making a toy dataset, which will enable us to better understand the network. Since we have full control over the data, we can see how changes impact MIMO.\n",
    "\n",
    "The most basic problems we can solve with a MIMO-configured neural network is regression. Classification would also be a somewhat simple problem to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate training data from the function\n",
    "$$y = x + 0.3 \\sin{(2\\pi (x+ \\epsilon)) + 0.3 \\sin{(4\\pi (x+\\epsilon))}} $$\n",
    "as described by Blundell et al.\n",
    "\n",
    "We use $N_{train}=500$ training points and $N_{test}$ testing points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data function\n",
    "f = lambda x, epsilon: x + 0.3 * np.sin(2*np.pi * (x+epsilon)) + 0.3 * np.sin(4 * np.pi * (x+epsilon)) + epsilon\n",
    "\n",
    "def generate_data(N, lower, upper, std, f=f):\n",
    "    # create data\n",
    "    x = np.linspace(lower, upper, N)\n",
    "\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        epsilon = np.random.normal(0, std)\n",
    "        y.append(f(x[i], epsilon))\n",
    "    return x, y\n",
    "\n",
    "# Generate train data\n",
    "N_train = 2000\n",
    "x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate validation data\n",
    "N_val = 500\n",
    "x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "# Generate test data\n",
    "N_test = 500\n",
    "x_test, y_test = generate_data(N_test, lower=-0.5, upper=1.5, std=0.02)\n",
    "\n",
    "line = f(x_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a custom toy dataset class for the MIMO model. During training, the $M$ subnetworks in the model each get a different batch of training samples. We can do this by multiplying the batch size with a factor $M$ such that the effective batch size is:\n",
    "$$\n",
    "batch\\_size^* = \\frac{batch\\_size}{M}\n",
    "$$\n",
    "This is implemented using a custom collate function.\n",
    "\n",
    "During inference, the $M$ subnetworks in the model each get the same input. We implement this by multiplying the input $M$ times using another collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def train_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for training MIMO\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "    \n",
    "    x_chunks = torch.stack(torch.chunk(torch.tensor(x), M, dim=0), dim=1)\n",
    "    y_chunks = torch.stack(torch.chunk(torch.tensor(y), M, dim=0), dim=1)\n",
    "\n",
    "    return x_chunks, y_chunks\n",
    "\n",
    "def test_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for testing MIMO\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "    x = torch.tensor(x)[:,None].repeat(1,M)\n",
    "    y = torch.tensor(y)[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def naive_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for naive multiheaded model\"\"\"\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "    x = torch.tensor(x)[:,None]\n",
    "    y = torch.tensor(y)[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "M = 2\n",
    "traindata = ToyDataset(x, y)\n",
    "trainloader = DataLoader(traindata, batch_size=60*M, shuffle=True, collate_fn=lambda x: train_collate_fn(x, M), drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "naivetrainloader = DataLoader(traindata, batch_size=60, shuffle=True, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "valdata = ToyDataset(x_val, y_val)\n",
    "valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)\n",
    "naivevalloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "testdata = ToyDataset(x_test, y_test)\n",
    "testloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)\n",
    "naivetestloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader properties\n",
    "Testing shape of output from train and test loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``trainloader`` should output a batch where columns are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_train,y_train in trainloader:\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    break\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIMO dataloaders should output a batch with size $(batch\\_size, M)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_x, test_y in testloader:\n",
    "    print(test_x.shape)\n",
    "    print(test_y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader for the naive multiheaded model should output a batch with size $(batch\\_size, 1)$ for the features $x$ and labels (batch_size, M) for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for naive_x_train, naive_y_train in naivetrainloader:\n",
    "    print(naive_x_train.shape)\n",
    "    print(naive_y_train.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "\n",
    "ax.plot(x, y, '.', label='Train data', color='orange')\n",
    "ax.plot(x_test, y_test, '.', label='Test data', color='blue')\n",
    "ax.plot(x_test, line, '--', label='true function', color='red')\n",
    "ax.legend()\n",
    "ax.set_title(\"One-dimensional dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Building the model***\n",
    "\n",
    "The model is a MIMO configured neural network. To keep it simple for now we choose to have $M=2$, so we use 2 subnetworks.\n",
    "The neural network will consist of two feed-forward aka linear layers with 32 and 128 hidden units in each. For activation function we will use ReLU as it is the norm within the field.\n",
    "\n",
    "We choose the number of hidden units we did, so that our architecture would be the same as the one given in the MIMO paper when they fit a MIMO model to this synthetic data.\n",
    "\n",
    "We make sure that we can get both the averaged output (ensemble output) and individual ensemble member output. This will help us illustrate the diversity of the ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "hidden_units = 32\n",
    "hidden_units2 = 128\n",
    "\n",
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\n",
    "class MIMONetwork(nn.Module):\n",
    "    def __init__(self, n_subnetworks):\n",
    "        super().__init__()\n",
    "        self.n_subnetworks = n_subnetworks\n",
    "        self.model = torch.nn.Sequential(\n",
    "            nn.Linear(self.n_subnetworks, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units,hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units2, self.n_subnetworks)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        individual_outputs = self.model(x)\n",
    "        output = torch.mean(individual_outputs, dim=1)\n",
    "        return output, individual_outputs\n",
    "\n",
    "MIMO_model = MIMONetwork(n_subnetworks=M)\n",
    "MIMO_model.apply(init_weights)\n",
    "\n",
    "class NaiveNetwork(nn.Module):\n",
    "    def __init__(self, n_subnetworks):\n",
    "        super().__init__()\n",
    "        self.n_subnetworks = n_subnetworks\n",
    "        self.model = torch.nn.Sequential(\n",
    "            nn.Linear(1, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units,hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units2, self.n_subnetworks)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        individual_outputs = self.model(x)\n",
    "        output = torch.mean(individual_outputs, dim=1)\n",
    "        return output, individual_outputs\n",
    "\n",
    "naive_model = NaiveNetwork(n_subnetworks=M)\n",
    "naive_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight distribution\n",
    "\n",
    "weights_mimo = []\n",
    "for param in MIMO_model.parameters():\n",
    "    weights_mimo.extend(param.flatten().detach().numpy())\n",
    "\n",
    "weights_naive = []\n",
    "for param in naive_model.parameters():\n",
    "    weights_naive.extend(param.flatten().detach().numpy())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.hist(weights_mimo, bins=50, alpha=0.5, label='MIMO_model')\n",
    "ax1.set_xlabel('Weight Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Weight Distribution - MIMO Model')\n",
    "\n",
    "ax2.hist(weights_naive, bins=50, alpha=0.5, label='naive_model')\n",
    "ax2.set_xlabel('Weight Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Weight Distribution - Naive Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions ðŸ¤–\n",
    "\n",
    "def train(model, optimizer, trainloader, valloader, epochs=500, model_name='MIMO', val_every_n_epochs=10):\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_,y_ = x_.float(), y_.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, individual_outputs = model(x_)\n",
    "            loss = nn.functional.mse_loss(individual_outputs, y_)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())  \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float(), val_y.float()\n",
    "                    val_output, val_individual_outputs = model(val_x)\n",
    "                    val_loss = nn.functional.mse_loss(val_individual_outputs, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, val_losses\n",
    "\n",
    "def get_train_val_dataloaders(N_train=500, N_val=200, is_naive=False):\n",
    "    # Generate train data\n",
    "    x, y = generate_data(N_train, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "    # Generate validation data\n",
    "    x_val, y_val = generate_data(N_val, lower=-0.25, upper=1, std=0.02)\n",
    "\n",
    "    # make dataset and get dataloaders\n",
    "    traindata = ToyDataset(x, y)\n",
    "    valdata = ToyDataset(x_val, y_val)\n",
    "    \n",
    "    if is_naive:\n",
    "        trainloader = DataLoader(traindata, batch_size=60, shuffle=True, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "        valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "    else:\n",
    "        trainloader = DataLoader(traindata, batch_size=60*M, shuffle=True, collate_fn=lambda x: train_collate_fn(x, M), drop_last=True)\n",
    "        valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)  \n",
    "    \n",
    "    return trainloader, valloader\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train one MIMO model and one Naive model with $M=[2,3,,4,5]$ subnetworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MIMO model with M = [2,3,4, 5]\n",
    "\n",
    "Ms = [2, 3, 4, 5]\n",
    "\n",
    "for M in Ms:\n",
    "    MIMO_model = MIMONetwork(n_subnetworks=M)\n",
    "    MIMO_model.apply(init_weights)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(MIMO_model.parameters(), lr=3e-4)\n",
    "\n",
    "    trainloader = DataLoader(traindata, batch_size=60*M, shuffle=True, collate_fn=lambda x: train_collate_fn(x, M), drop_last=True)\n",
    "    valloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    losses, val_losses = train(MIMO_model, optimizer, epochs=5000, trainloader=trainloader, valloader=valloader, val_every_n_epochs=2, model_name=f'mimo_ensembles/MIMO_{M}_members')\n",
    "    # plot loss \n",
    "    plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Naive model with M = [2,3,4]\n",
    "Ms = [2, 3, 4, 5]\n",
    "\n",
    "for M in Ms:\n",
    "    naive_model = NaiveNetwork(n_subnetworks=M)\n",
    "    naive_model.apply(init_weights)\n",
    "    optimizer = torch.optim.Adam(naive_model.parameters(), lr=3e-4)\n",
    "\n",
    "    naivetrainloader = DataLoader(traindata, batch_size=60, shuffle=True, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "    naivevalloader = DataLoader(valdata, batch_size=60, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    losses, val_losses = train(naive_model, optimizer, epochs=5000, trainloader=naivetrainloader, valloader=naivevalloader, val_every_n_epochs=2, model_name=f'naive_ensembles/Naive_{M}_members')\n",
    "    # plot loss \n",
    "    plot_loss(losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "We test the model on the test set and try to predict the function. We predict the mean outputs (of ensemble member) and of individual members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, testloader):\n",
    "    predictions = []\n",
    "    pred_individual = []\n",
    "\n",
    "    for test_x, test_y in testloader:\n",
    "        output, individual_outputs = model(test_x.float())\n",
    "        \n",
    "        predictions.extend(list(output.detach().numpy()))\n",
    "        pred_individual.extend(list(individual_outputs.detach().numpy()))\n",
    "\n",
    "    return np.array(predictions), np.array(pred_individual)\n",
    "\n",
    "# get predictions and individual predictions for MIMO and Naive models\n",
    "def get_mimo_predictions(model_path, Ms, testdata, N_test=200):\n",
    "\n",
    "    predictions_matrix = np.zeros((len(model_path), N_test))\n",
    "    pred_individual_list = []\n",
    "\n",
    "    for i, model in enumerate(model_path):\n",
    "\n",
    "        M = Ms[i]\n",
    "        testloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=lambda x: test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "        model = torch.load(model)\n",
    "        predictions, pred_individual = inference(model, testloader)\n",
    "\n",
    "        predictions_matrix[i, :] = predictions\n",
    "        pred_individual_list.append(pred_individual)\n",
    "            \n",
    "    return predictions_matrix, pred_individual_list\n",
    "\n",
    "def get_naive_predictions(model_path, Ms, testdata, N_test=200):\n",
    "\n",
    "    predictions_matrix = np.zeros((len(model_path), N_test))\n",
    "    pred_individual_list = []\n",
    "\n",
    "    for i, model in enumerate(model_path):\n",
    "\n",
    "        M = Ms[i]\n",
    "        testloader = DataLoader(testdata, batch_size=N_test, shuffle=False, collate_fn=lambda x: naive_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "        model = torch.load(model)\n",
    "        predictions, pred_individual = inference(model, testloader)\n",
    "\n",
    "        predictions_matrix[i, :] = predictions\n",
    "        pred_individual_list.append(pred_individual)\n",
    "            \n",
    "    return predictions_matrix, pred_individual_list\n",
    "\n",
    "Ms = [2, 3, 4]\n",
    "\n",
    "mimo_path = [os.path.join(\"mimo_ensembles/\", model) for model in ['MIMO_2_members.pt', 'MIMO_3_members.pt', 'MIMO_4_members.pt']]\n",
    "mimo_pred_matrix, mimo_individual_list = get_mimo_predictions(mimo_path, Ms, testdata, N_test)\n",
    "mimo_stds = np.array([np.std(pred, axis=1, ddof=1) for pred in mimo_individual_list])\n",
    "\n",
    "naive_path = [os.path.join(\"naive_ensembles/\", model) for model in ['Naive_2_members.pt', 'Naive_3_members.pt', 'Naive_4_members.pt']]\n",
    "naive_pred_matrix, naive_individual_list = get_naive_predictions(naive_path, Ms, testdata, N_test)\n",
    "naive_stds = np.array([np.std(pred, axis=1, ddof=1) for pred in naive_individual_list])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "We need to analyse the trained network to see if the MIMO network works as desired. We plot the mean predictions of each model and the 95% confidence intervals using the standard deviation computed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1,2, figsize=(18,12))\n",
    "\n",
    "### plot mimo ###\n",
    "ax[0].plot(x, y, '.', label='Train data', color='orange')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].grid()\n",
    "ax[0].plot(x_test, line, '--', label='true function', color='red')\n",
    "# plot test data\n",
    "ax[0].plot(x_test, y_test, '.', label='Test data', color='black')\n",
    "\n",
    "# plot predicitons with confidence intervals\n",
    "for i in range(len(Ms)):\n",
    "    ax[0].plot(x_test, mimo_pred_matrix[i], '-', label=f'Mean MIMO Predictions with {Ms[i]} members', linewidth=2)\n",
    "    ax[0].fill_between(x_test, mimo_pred_matrix[i] - 1.96*mimo_stds[i], mimo_pred_matrix[i] + 1.96*mimo_stds[i], alpha=0.2, label=f'Confidence Interval with {Ms[i]} members')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "### naive ###\n",
    "ax[1].plot(x, y, '.', label='Train data', color='orange')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].grid()\n",
    "ax[1].plot(x_test, line, '--', label='true function', color='red')\n",
    "# plot test data\n",
    "ax[1].plot(x_test, y_test, '.', label='Test data', color='black')\n",
    "\n",
    "# # plot predicitons with confidence intervals\n",
    "for i in range(len(Ms)):\n",
    "    ax[1].plot(x_test, naive_pred_matrix[i], '-', label=f'Mean naive Predictions with {Ms[i]} members', linewidth=2)\n",
    "    ax[1].fill_between(x_test, naive_pred_matrix[i] - 1.96*naive_stds[i], naive_pred_matrix[i] + 1.96*naive_stds[i], alpha=0.2, label=f'Confidence Interval with {Ms[i]} members')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the mean predictions for the MIMO and naive models with $M=[2,3,4]$ ensemble members, illustrated with a 95% confidence interval. \n",
    "The confidence interval is constructed using the variance of individual predictions for each subnetwork in a model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "We now move on to the classification task. To keep close to the MIMO paper we use CIFAR-10 for data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to follow the SmallCNN example from Fort et al. that is also used in the MIMO paper. Some parts are not well described in the paper, such as the exact structure of the input and output layers. So we have simply made some decision that we think are appropriate in those regards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load/download CIFAR10\n",
    "# https://github.com/kuangliu/pytorch-cifar/issues/19 normalisation values\n",
    "transform = transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "CIFAR_traindata = torchvision.datasets.CIFAR10(root=\"../data/\", train=True, transform = transform, download=False)\n",
    "CIFAR_train, CIFAR_val = torch.utils.data.random_split(CIFAR_traindata, [int(len(CIFAR_traindata)*0.9), int(len(CIFAR_traindata)*0.1)])\n",
    "\n",
    "CIFAR_test = torchvision.datasets.CIFAR10(root=\"../data/\", train=False, transform = transform, download=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images from the CIFAR10 dataset are 32x32 colour images. Below, some examples are visualised with their label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_transform = transforms.Normalize(mean=[-0.4914/0.247, -0.4822/0.243, -0.4465/0.261], std=[1/0.247, 1/0.243, 1/0.261])\n",
    "label_dict = {0: \"airplane\", \n",
    "              1: \"automobile\",\n",
    "              2: \"bird\",\n",
    "              3: \"cat\",\n",
    "              4: \"deer\",\n",
    "              5: \"dog\",\n",
    "              6: \"frog\",\n",
    "              7: \"horse\",\n",
    "              8: \"ship\",\n",
    "              9: \"truck\"}\n",
    "\n",
    "# plot some images\n",
    "fig, ax = plt.subplots(1,5, figsize=(18,12))\n",
    "for i in range(5):\n",
    "    x, y = CIFAR_train[i+25]\n",
    "    img = inv_transform(x) # unnormalise\n",
    "    ax[i].imshow(img.permute(1,2,0))\n",
    "    ax[i].set_title(f'Label: {label_dict[y]}')\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Github repository](https://github.com/google/edward2/blob/7a11c9782b1e72eef8e2f4fe7914827b799b692f/experimental/mimo/imagenet_model.py#L132-L138) they use the following operations to manipulate the input shape of ImageNet images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "input_shape = (M, 32, 32, 3)\n",
    "input_shape = list(input_shape)\n",
    "inp = torch.ones(input_shape)\n",
    "x = inp.permute(1,2,3,0)\n",
    "x = torch.ones(list(input_shape[1:-1]) + [input_shape[-1] * M])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply this to the CIFAR10 dataset, we extrapolate that we need to concatenate the ``ensemble_size`` dimension and channel dimension. This is what we will do in the collate functions below.\n",
    "\n",
    "The collate function needs to be different for the CIFAR data. Pytorch has built-in CIFAR dataset so no dataset class is needed. The collate function is responsible for drawing M samples at a time so that each of the subnetworks gets a different input during training and the same input during inference.\n",
    "\n",
    "During training, the input image $x$ naturally has shape $(batch\\_size \\cdot M, C, H, W)$ and the input label has shape $(batch\\_size \\cdot M). With the collate fucntion during training, the input image and label is split into $M$ chunks and concatenated in the channel dimension (dim = 1). \n",
    "\n",
    "With the collate function during testing, the input image and label is duplicated $M$ times in dim 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collate functions\n",
    "def C_train_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for training MIMO on CIFAR classification\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    x, y = torch.stack(list(x)), torch.tensor(y)\n",
    "    x = torch.cat(torch.chunk(x, M, dim=0), dim=1)\n",
    "    y = torch.stack(torch.chunk(y, M, dim=0), dim=1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def C_test_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for testing MIMO on CIFAR classification\"\"\"\n",
    "    \n",
    "    x, y = zip(*batch)\n",
    "    x, y = torch.stack(list(x)), torch.tensor(y)\n",
    "    x = x.repeat(1, M, 1, 1)\n",
    "    y = y[:,None].repeat(1,M)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def C_Naive_train_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for training Naive multiheaded on CIFAR classification\"\"\"\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    x, y = torch.stack(list(x)), torch.tensor(y)\n",
    "    y = y[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def C_Naive_test_collate_fn(batch, M):\n",
    "    \"\"\"Collate function for testing Naive multiheaded on CIFAR classsification\"\"\"\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "    x, y = torch.stack(list(x)), torch.tensor(y)\n",
    "    y = y[:,None].repeat(1,M)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is constructed in the following way:\n",
    "- The input is put through $3$ convolutions with channels going from $3 \\rightarrow 16$, $16 \\rightarrow 32$, $32 \\rightarrow 32$\n",
    "- the output from the convolutions should be shape $( batch\\_size, 32, 32, 32)$, because we use ``padding=1``. This is then flattened before being inputted into the linear layer\n",
    "- after convolution there are $2$ linear layers where the dimensions is reduced down to $M\\cdot 10$ - 10 for each class, for each subnetwork'\n",
    "\n",
    "In the forward pass, the output are LogSoftmax probabilities. \n",
    "\n",
    "A separate ``Ã¬nference`` function is created to output the actual predictions of 1) individual subnetworks and 2) the ensemble as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Network\n",
    "class C_MIMONetwork(nn.Module):\n",
    "    def __init__(self, n_subnetworks):\n",
    "        super().__init__()\n",
    "        self.n_subnetworks = n_subnetworks\n",
    "        self.in_channels = 3\n",
    "        self.channels1 = 16\n",
    "        self.channels2 = 32\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels*self.n_subnetworks, self.channels1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.channels1, self.channels2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.channels2, self.channels2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output = torch.nn.Sequential(\n",
    "            nn.Linear(self.channels2 * 32 * 32, 128), # dim: self.channels2 x width x height\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, self.n_subnetworks*10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # reshape to fit into linear layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.output(x)\n",
    "\n",
    "        # reshape to batch_size x M x 10\n",
    "        x = x.reshape(x.size(0), self.n_subnetworks, -1)\n",
    "        # Log-softmax over the last dimension (because we are using NLL loss)\n",
    "        x = nn.LogSoftmax(dim=2)(x)\n",
    "\n",
    "        # get individual outputs \n",
    "        # during training, we want each subnetwork to to clasify their corresponding inputs\n",
    "        individual_outputs = torch.argmax(x, dim=2) # dim : batch_size x M\n",
    "        \n",
    "        # get ensemble output\n",
    "        # during inference, we mean the softmax probabilities over all M subnetworks and then take the argmax\n",
    "        output = torch.mean(x, dim=1).argmax(dim=1) # dim : batch_size\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        return x, output, individual_outputs\n",
    "\n",
    "    # def inference(self, x):\n",
    "\n",
    "    #     x = self.conv(x)\n",
    "    #     # reshape to fit into linear layer\n",
    "    #     x = x.reshape(x.size(0), -1)\n",
    "    #     x = self.output(x)\n",
    "\n",
    "    #     # reshape to batch_size x M x 10\n",
    "    #     x = x.reshape(x.size(0), M, -1)\n",
    "    #     # Log-softmax over the last dimension (because we are using NLL loss)\n",
    "    #     x = nn.LogSoftmax(dim=2)(x)\n",
    "\n",
    "    #     # get individual outputs \n",
    "    #     # during training, we want each subnetwork to to clasify their corresponding inputs\n",
    "    #     individual_outputs = torch.argmax(x, dim=2) # dim : batch_size x M\n",
    "        \n",
    "    #     # get ensemble output\n",
    "    #     # during inference, we mean the softmax probabilities over all M subnetworks and then take the argmax\n",
    "    #     output = torch.mean(x, dim=1).argmax(dim=1) # dim : batch_size\n",
    "\n",
    "    #     return output, individual_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Network\n",
    "class C_NaiveNetwork(nn.Module):\n",
    "    def __init__(self, n_subnetworks):\n",
    "        super().__init__()\n",
    "        self.n_subnetworks = n_subnetworks\n",
    "        self.in_channels = 3\n",
    "        self.channels1 = 16\n",
    "        self.channels2 = 32\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.channels1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.channels1, self.channels2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.channels2, self.channels2, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output = torch.nn.Sequential(\n",
    "            nn.Linear(self.channels2 * 32 * 32, 128), # dim: self.channels2 x width x height\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, self.n_subnetworks*10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # reshape to fit into linear layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.output(x)\n",
    "\n",
    "        # reshape to batch_size x M x 10\n",
    "        x = x.reshape(x.size(0), self.n_subnetworks, -1)\n",
    "        # Log-softmax over the last dimension (because we are using NLL loss)\n",
    "        x = nn.LogSoftmax(dim=2)(x)\n",
    "        \n",
    "        # get individual outputs \n",
    "        # during training, we want each subnetwork to to clasify their corresponding inputs\n",
    "        individual_outputs = torch.argmax(x, dim=2) # dim : batch_size x M\n",
    "        \n",
    "        # get ensemble output\n",
    "        # during inference, we mean the softmax probabilities over all M subnetworks and then take the argmax\n",
    "        output = torch.mean(x, dim=1).argmax(dim=1) # dim : batch_size\n",
    "\n",
    "        x = x.permute(1,0,2)\n",
    "\n",
    "        return x, output, individual_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "# train on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(model, optimizer, trainloader, valloader, epochs=500, model_name='MIMO', val_every_n_epochs=10, checkpoint_every_n_epochs=20, loss_fn = nn.NLLLoss(reduction='mean'), device='cpu'):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    val_checkpoint_list = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            x_,y_ = x_.float().to(device), y_.long().to(device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_prob, _, _ = model(x_)\n",
    "            \n",
    "            # sum loss per subnetwork\n",
    "            # mean is already taken over the batch, because we use reduction = 'mean' in the loss function\n",
    "            loss = 0\n",
    "            for log_p, y in zip(log_prob, y_.T):\n",
    "                # print(log_p.shape)\n",
    "                # print(y.shape)\n",
    "                loss += loss_fn(log_p, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())  \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float().to(device), val_y.long().to(device)\n",
    "                    log_prob, _, _ = model(val_x)\n",
    "                \n",
    "                    val_loss = 0\n",
    "                    for log_p, y in zip(log_prob, val_y.T):\n",
    "                        val_loss += loss_fn(log_p, y)\n",
    "\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "                if (e+1) % checkpoint_every_n_epochs == 0:\n",
    "                    val_checkpoint_list.append(log_prob[0,:,:])\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}_{model.n_subnetworks}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "    torch.save(torch.stack(val_checkpoint_list), f'{model_name}_{model.n_subnetworks}_checkpoints.pt')\n",
    "\n",
    "    return losses, val_losses, val_checkpoint_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classification MIMO model with M = [2,3,4]\n",
    "\n",
    "loss_fn = nn.NLLLoss(reduction='mean')\n",
    "Ms = [2, 3, 4]\n",
    "batch_size = 512\n",
    "\n",
    "for M in Ms:\n",
    "    C_MIMO_model = C_MIMONetwork(n_subnetworks=M)\n",
    "    C_MIMO_model.apply(init_weights)\n",
    "    C_MIMO_model = C_MIMO_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(C_MIMO_model.parameters(), lr=3e-4)\n",
    "    \n",
    "    C_trainloader = DataLoader(CIFAR_train, batch_size=batch_size*M, shuffle=True, collate_fn=lambda x: C_train_collate_fn(x, M), drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "    C_valloader = DataLoader(CIFAR_val, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    losses, val_losses, val_checkpoint_list = train_classification(C_MIMO_model, optimizer, C_trainloader, C_valloader, epochs=30, model_name='C_MIMO', val_every_n_epochs=2, checkpoint_every_n_epochs=20, loss_fn = loss_fn, device=device)\n",
    "    # plot loss \n",
    "    plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classification Naive model with M = [2,3,4]\n",
    "\n",
    "loss_fn = nn.NLLLoss(reduction='mean')\n",
    "Ms = [2, 3, 4]\n",
    "batch_size = 512\n",
    "\n",
    "for M in Ms:\n",
    "    C_Naive_model = C_NaiveNetwork(n_subnetworks=M)\n",
    "    C_Naive_model.apply(init_weights)\n",
    "    C_Naive_model = C_Naive_model.to(device)\n",
    "    \n",
    "    Naive_optimizer = torch.optim.SGD(C_Naive_model.parameters(), lr=1e-3, weight_decay=3e-4)\n",
    "    \n",
    "    C_Naive_trainloader = DataLoader(CIFAR_train, batch_size=batch_size, shuffle=True, collate_fn=lambda x: C_Naive_train_collate_fn(x, M), drop_last=True, worker_init_fn=seed_worker, generator=g)\n",
    "    C_Naive_valloader = DataLoader(CIFAR_val, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_Naive_test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    naive_losses, naive_val_losses, val_checkpoint_list = train_classification(C_Naive_model, Naive_optimizer, C_Naive_trainloader, C_Naive_valloader, epochs=30, model_name=\"C_Naive\", val_every_n_epochs=2, checkpoint_every_n_epochs=20, loss_fn = loss_fn, device=device)\n",
    "    # plot loss \n",
    "    plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will predict on the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "C_testloader = DataLoader(CIFAR_test, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_test_collate_fn(x, M), drop_last=False)\n",
    "C_Naive_testloader = DataLoader(CIFAR_test, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_Naive_test_collate_fn(x, M), drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight distribution\n",
    "\n",
    "weights_mimo = []\n",
    "for param in C_MIMO_model.parameters():\n",
    "    weights_mimo.extend(param.flatten().cpu().detach().numpy())\n",
    "\n",
    "weights_naive = []\n",
    "for param in C_Naive_model.parameters():\n",
    "    weights_naive.extend(param.flatten().cpu().detach().numpy())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.hist(weights_mimo, bins=50, alpha=0.5, label='MIMO_model')\n",
    "ax1.set_xlabel('Weight Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Weight Distribution - Classification MIMO Model')\n",
    "\n",
    "ax2.hist(weights_naive, bins=50, alpha=0.5, label='naive_model')\n",
    "ax2.set_xlabel('Weight Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Weight Distribution - Classification Naive Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_mimo_path = [os.path.join(\"C_mimo_ensembles/\", model) for model in ['C_MIMO_2.pt', 'C_MIMO_3.pt', 'C_MIMO_4.pt']]\n",
    "\n",
    "for i, model in enumerate(C_mimo_path):\n",
    "    M = i+2\n",
    "    C_MIMO_model = torch.load(model)\n",
    "    C_MIMO_model = C_MIMO_model.to(device)\n",
    "\n",
    "    C_testloader = DataLoader(CIFAR_test, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    confidence = []\n",
    "\n",
    "    for x_test, y_test in C_testloader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        _, output, individual_outputs = C_MIMO_model.forward(x_test)\n",
    "        \n",
    "        predictions.extend(list(output.cpu().detach().numpy()))\n",
    "        ground_truth.extend(list(y_test[:,0].cpu().detach().numpy()))\n",
    "\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(ground_truth))\n",
    "    print(f\"Accuracy for MIMO with {M} members:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_naive_path = [os.path.join(\"C_naive_ensembles/\", model) for model in ['C_Naive_2.pt', 'C_Naive_3.pt', 'C_Naive_4.pt']]\n",
    "\n",
    "for i, model in enumerate(C_naive_path):\n",
    "    M = i+2\n",
    "    C_Naive_model = torch.load(model)\n",
    "    C_Naive_model = C_Naive_model.to(device)\n",
    "\n",
    "    C_Naive_testloader = DataLoader(CIFAR_test, batch_size=batch_size, shuffle=False, collate_fn=lambda x: C_Naive_test_collate_fn(x, M), drop_last=False)\n",
    "\n",
    "    naive_predictions = []\n",
    "    naive_ground_truth = []\n",
    "    naive_confidence = []\n",
    "\n",
    "    for x_test, y_test in C_Naive_testloader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        _, output, individual_outputs = C_Naive_model.forward(x_test)\n",
    "        \n",
    "        naive_predictions.extend(list(output.cpu().detach().numpy()))\n",
    "        naive_ground_truth.extend(list(y_test[:,0].cpu().detach().numpy()))\n",
    "\n",
    "    accuracy = np.mean(np.array(naive_predictions) == np.array(naive_ground_truth))\n",
    "    print(f\"Accuracy for naive model with {M} members:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plots for Image Classifiers***\n",
    "\n",
    "We want to illustrate the overconfidence of the Image classifier models. To do this we make reliability diagrams as described in Gou et al. \n",
    "\n",
    "The reliability diagram has accuracy on the y-axis and confidence on the x-axis. We sample some test points, then bin them based on the models confidence in its prediction for those points. Thus we will have accuracy sorted in bins like so:\n",
    "$$acc(B_m)=\\frac{1}{|B_m|}\\sum_{i\\in B_m} \\mathbf{1}(\\hat{y}_i=y_i)$$\n",
    "where $\\mathbf{1}(\\hat{y}_i=y_i)$ is an indicator function for a correct prediction and $B_m$ is the m-th bin of $M$ bins with width $\\frac{1}{M}$.\n",
    "\n",
    "The confidence is likewise calculated as \n",
    "$$conf(B_m)=\\frac{1}{|B_m|}\\sum_{i\\in B_m} \\hat{p}_i$$\n",
    "where $\\hat{p_i}$ is the confidence for sample $i$. In this case, since we have no better measure, the softmax probabilities will be used as confidence scores.\n",
    "\n",
    "A perfectly calibrated model will have $acc(B_m) = conf(B_m)$ but since neural networks are typically overconfident we will expect to see $acc(B_m) < conf(B_m)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for generating reliability diagram:\n",
    "bins_range = np.arange(0, 1, 0.1)\n",
    "conf_step_height = np.zeros(10)\n",
    "acc_step_height = np.zeros(10)\n",
    "for i in range(len(bins_range)-1):\n",
    "    loc = np.where(confidence>=bins_range[i] and confidence<bins_range[i+1])\n",
    "    conf_step_height[i] = np.mean(confidence[loc])\n",
    "    acc_step_height[i] = np.mean(accuracy[loc])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "naive_conf_step_height = np.zeros(10)\n",
    "naive_acc_step_height = np.zeros(10)\n",
    "for i in range(len(bins_range)-1):\n",
    "    loc = np.where(naive_confidence>=bins_range[i] and naive_confidence<bins_range[i+1])\n",
    "    naive_conf_step_height[i] = np.mean(confidence[loc])\n",
    "    naive_acc_step_height[i] = np.mean(accuracy[loc])\n",
    "\n",
    "\n",
    "\n",
    "ax, fig = plt.subplots(2,1, sharey=True)\n",
    "ax.set_xlabel(\"Confidence\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax[0].stairs(conf_step_height, bins_range, hatch=\"//\")\n",
    "ax[0].stairs(acc_step_height, bins_range, fill = True)\n",
    "ax[0].set_title(\"Reliability Plot MIMO\")\n",
    "\n",
    "ax[1].stairs(conf_step_height, bins_range, hatch=\"//\", legend='Gap')\n",
    "ax[1].stairs(acc_step_height, bins_range, fill = True, legend=\"Outputs\")\n",
    "ax[1].set_title(\"Reliability Plot Naive Multiheaded\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to observe the diversity of the subnetworks in the MIMO model, we use function space plots like in Fort et al.\n",
    "This lets us visualize how each subnetwork moves in function space.\n",
    "\n",
    "The plots are made with the following steps:\n",
    "- At certain model checkpoints, get softmax predictions and use tSNE to reduce the softmaxoutput to 2d so that you can plot it.\n",
    "- first separate checkpoint output so that each subnetwork is isolated\n",
    "- then apply the same t-SNE algorithm to all of them\n",
    "- then plot them together\n",
    "\n",
    "tips:\n",
    "- TNSE works best with a reasonable dimensionality (around 50 dimensions) [citation needed] so we limit the number of predictions. We have a large batch size and 10 class prediction pr data-point. Consider this when selecting how many predictions to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_checkpoint_list.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_checkpoint_list = torch.stack(val_checkpoint_list[:,:5,:]).flatten()\n",
    "tSNE = TSNE(val_checkpoint_list.shape)\n",
    "val_checkpoint_list2d = tSNE.fit_transform(val_checkpoint_list)\n",
    "plt.plot(val_checkpoint_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(val_checkpoint_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512*392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_checkpoint_list[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastersthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
