{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian convolutional layers\n",
    "The goal of this notebook is to implement the Bayesian convolutional layers proposed by [Shridkar et al.](https://github.com/kumar-shridhar/PyTorch-BayesianCNN) and [Seligmann et al.](https://github.com/Feuermagier/Beyond_Deep_Ensembles/tree/main). \n",
    "\n",
    "Rather than learning the distribution of weights in the filter, the goal is to learn the distribution of activations after appyling one convolutional layer. This approach is described in the paper \"Variational Dropout and the Local Reparameterization Trick\" by [Kingma et al.](https://arxiv.org/pdf/1506.02557). \n",
    "\n",
    "Consider the input feature matrix $\\mathbf{A}$ of size $M \\times 1000$ and a weight matrix $\\mathbf{W}$ of size $1000 \\times 1000$. When multiplied together, the resulting matrix $\\mathbf{B}$ is:\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{AW}    \n",
    "$$\n",
    "where $\\mathbf{B}$ are called the activations. Rather than trying to learn the distribution of the weights in $\\mathbf{W}$, we learn the activations $\\mathbf{B}$ directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian prior\n",
    "The prior that we implemented during the Master's thesis is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ScaleMixturePrior():\n",
    "    def __init__(self, pi=0.5, sigma1=torch.exp(torch.tensor(0)), sigma2=torch.tensor(0.3), device='cpu'):\n",
    "        self.device = device\n",
    "        self.pi = pi\n",
    "        self.mu = torch.tensor(0)\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "    def prob(self, w, sigma):\n",
    "    \n",
    "        return (1 / (sigma * torch.sqrt(torch.tensor(2 * np.pi)))) * torch.exp(-0.5 * torch.pow((w - self.mu), 2) / torch.pow(sigma, 2))\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        prob1 = self.prob(w, self.sigma1)\n",
    "        prob2 = self.prob(w, self.sigma2)\n",
    "\n",
    "        return torch.log(self.pi * prob1 + ((1 - self.pi) * prob2)).sum() if self.sigma2.item() > 0 else torch.log(prob1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Seligmann et al., the standard today is not to use the scale mixture prior proposed by Blundell et al. in the original paper. We take this into account and implement the Gaussian prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/Feuermagier/Beyond_Deep_Ensembles/tree/main\n",
    "\n",
    "class GaussianPrior:\n",
    "    def __init__(self, sigma1):\n",
    "        self.sigma1 = sigma1\n",
    "        self.dist = torch.distributions.Normal(0, sigma1)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self.dist.log_prob(x).sum()\n",
    "    \n",
    "class Gaussian():\n",
    "    def __init__(self, mu, rho, device='cpu'):\n",
    "        self.device = device\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.init_distribution()\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "    \n",
    "    def init_distribution(self):\n",
    "        self.normal = torch.distributions.Normal(self.mu, self.sigma)\n",
    "    \n",
    "    def rsample(self):\n",
    "        self.init_distribution()\n",
    "        return self.normal.rsample()\n",
    "    \n",
    "    def log_prob(self, w):\n",
    "        return self.normal.log_prob(w).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test that the two priors are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Mixture Prior with \\pi=1:  -11571.013671875\n",
      "Gaussian Prior:  -11571.013671875\n"
     ]
    }
   ],
   "source": [
    "prior1 = ScaleMixturePrior(pi=1, sigma1=torch.tensor(40.82))\n",
    "prior2 = GaussianPrior(sigma1=torch.tensor(40.82))\n",
    "\n",
    "test_tensor = torch.randn(2500)\n",
    "\n",
    "print(\"Scale Mixture Prior with \\pi=1: \", prior1.log_prob(test_tensor).item())\n",
    "print(\"Gaussian Prior: \", prior2.log_prob(test_tensor).item())\n",
    "\n",
    "# sometimes, there is a minimal difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BayesianConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, use_bias=True, device='cpu', sigma1=torch.tensor(1.0)):\n",
    "        super(BayesianConvLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).normal_(0, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-3, 3))\n",
    "        \n",
    "        if use_bias:\n",
    "            # initialise bias\n",
    "            self.bias_mu = nn.Parameter(torch.Tensor(out_channels).normal_(0, 0.1))\n",
    "            self.bias_rho = nn.Parameter(torch.Tensor(out_channels).uniform_(-3, 3))\n",
    "        else:\n",
    "            # set bias to \"None\"\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_rho', None)\n",
    "\n",
    "        # initialise variational posteriors\n",
    "        self.weight_posterior = Gaussian(self.weight_mu, self.weight_rho, device=device)\n",
    "        self.bias_posterior = Gaussian(self.bias_mu, self.bias_rho, device=device)\n",
    "\n",
    "        # initialise priors\n",
    "        self.weight_prior = GaussianPrior(sigma1=sigma1)\n",
    "        self.bias_prior = GaussianPrior(sigma1=sigma1)\n",
    "\n",
    "        self.log_prior = 0 \n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Taken from: https://github.com/Feuermagier/Beyond_Deep_Ensembles/blob/b805d6f9de0bd2e6139237827497a2cb387de11c/src/algos/util.py#L185\n",
    "\n",
    "        activation_mean = F.conv2d(x, self.weight_mu, self.bias_mu if self.use_bias else None, self.stride, self.padding, self.dilation)\n",
    "        actiation_var = F.conv2d((x**2).clamp(1e-4), (F.softplus(self.weight_rho)**2).clamp(1e-4), (F.softplus(self.bias_rho)**2).clamp(1e-4) if self.use_bias else None, self.stride, self.padding, self.dilation)\n",
    "        activation_std = torch.sqrt(actiation_var)\n",
    "\n",
    "        epsilon = torch.empty_like(activation_mean).normal_(0,1)   \n",
    "\n",
    "        w = self.weight_mu \n",
    "        b = self.bias_mu if self.use_bias else None\n",
    "\n",
    "        output = activation_mean + activation_std * epsilon\n",
    "\n",
    "        self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b) if self.use_bias else self.weight_prior.log_prob(w)\n",
    "        self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b) if self.use_bias else self.weight_posterior.log_prob(w)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BayesianConvLayer\n",
    "conv_layer = BayesianConvLayer(3, 64, 3, use_bias=True)\n",
    "noise_image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "output = conv_layer(noise_image)\n",
    "assert output.shape == (1, 64, 30, 30), \"Output shape is not correct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on CIFAR10 dataset\n",
    "We test the Bayesian convolutional layer on the CIFAR10 dataset. First, we build a normal CNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:09<00:00, 17790273.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# download CIFAR10 from PyTorch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup convolutional neural network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# train on CrossEntropyLoss and use SGD optimiser\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.197\n",
      "[1,  4000] loss: 1.833\n",
      "[1,  6000] loss: 1.685\n",
      "[1,  8000] loss: 1.591\n",
      "[1, 10000] loss: 1.535\n",
      "[1, 12000] loss: 1.491\n",
      "[2,  2000] loss: 1.440\n",
      "[2,  4000] loss: 1.398\n",
      "[2,  6000] loss: 1.382\n",
      "[2,  8000] loss: 1.344\n",
      "[2, 10000] loss: 1.332\n",
      "[2, 12000] loss: 1.313\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 54.69 %\n"
     ]
    }
   ],
   "source": [
    "# test network\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build a Bayesian classifier using the Bayesian convolutional layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
