{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/toydata/train_data.csv')\n",
    "df_val = pd.read_csv('../data/toydata/val_data.csv')\n",
    "df_test = pd.read_csv('../data/toydata/test_data.csv')\n",
    "\n",
    "x_train, y_train = df_train['x'].values, df_train['y'].values\n",
    "x_val, y_val = df_val['x'].values, df_val['y'].values\n",
    "x_test, y_test, line = df_test['x'].values, df_test['y'].values, df_test['line'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    return torch.tensor(x)[:,None], torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(0)\n",
    "\n",
    "traindata = ToyDataset(x_train, y_train)\n",
    "trainloader = DataLoader(traindata, batch_size=500, shuffle=True, collate_fn=collate_fn, drop_last=True, pin_memory=True)\n",
    "\n",
    "valdata = ToyDataset(x_val, y_val)\n",
    "valloader = DataLoader(valdata, batch_size=500, shuffle=True, collate_fn=collate_fn, drop_last=True, pin_memory=True)\n",
    "\n",
    "testdata = ToyDataset(x_test, y_test)\n",
    "testloader = DataLoader(testdata, batch_size=len(y_test), shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "NUM_BATCHES = len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a diagonal Gaussian distribution (zero covariance) as the variational posterior. Rather than using just $\\mu$ and $\\sigma$ as the variational parameters, the standard deviation is parameterised as:\n",
    "$$\n",
    "\\sigma = \\log{(1 + \\exp{(\\rho)})}\n",
    "$$\n",
    "such that $\\sigma$ is always non-negative. The variational parameters are then $\\mathbf{\\theta} = (\\mu, \\rho)$. \n",
    "\n",
    "The code blocks in the following sections are inspired by:\n",
    "\n",
    "https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "https://colab.research.google.com/drive/1K1I_UNRFwPt9l6RRkp8IYg1504PR9q4L#scrollTo=ASGi2Ecx5G-F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian():\n",
    "    def __init__(self, mu, rho, device='cpu'):\n",
    "        self.device = device\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(torch.tensor(0.0).to(self.device), torch.tensor(1.0).to(self.device))\n",
    "        \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def rsample(self):\n",
    "        epsilon = self.normal.sample(self.rho.size()) \n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        return self.normal.log_prob(w).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Gaussian`` class is a simple class that allows us to sample from a Gaussian distribution, representing the variational posterior.\n",
    "\n",
    "The function ``sigma`` computes the standard deviation $\\sigma$ for a given $\\rho$ value.\n",
    "The function ``sample`` allows us to sample from the approximate posterior, using the reparametrisation trick with $\\mu$ and $\\sigma$. \n",
    "The function ``log_prob`` computes the log-probability density function for a normal distribution wtih mean $\\mu$ and standard devation $\\sigma$ (derivation below):\n",
    "\n",
    "The probability density function for the weights $\\mathbf{w}$ given the variational parameters $\\mathbf{\\theta} = (\\mu, \\rho)$ of a Gaussian distribution is given as\n",
    "\\begin{align*}\n",
    "q(\\mathbf{w|\\mathbf{\\theta}}) &= \\prod_j \\mathcal{N}(w_j | \\mu, \\sigma) \\\\\n",
    "& = \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2}\n",
    "\\end{align*}\n",
    "Then taking the log, we get:\n",
    "\\begin{align*}\n",
    "\\log{q(\\mathbf{w}|\\mathbf{\\theta})} &=  \\log{  \\left( \\prod_j \\frac{1}{\\sigma \\sqrt{(2\\pi)}} \\exp{-\\frac{1}{2} \\left( \\frac{w_j - \\mu}{\\sigma}    \\right)^2} \\right)} \\\\\n",
    "           &= \\sum_j \\log{(1)} - \\log{\\left(\\sigma \\sqrt(2 \\pi)\\right) - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2 } \\\\\n",
    "           &= \\sum_j -\\log{(\\sigma)} - \\frac12 \\log{(2\\pi)} - \\frac12 \\left( \\frac{w_j - \\mu}{\\sigma} \\right)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior that was proposed in the paper by Blundell et al. is a Gaussian mixture prior over the weights $\\mathbf{w}$. \n",
    "$$\n",
    "P(\\mathbf{w}) = \\prod_j \\pi \\mathcal{N}(w_j|0, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(w_j| 0, \\sigma_2^2) \n",
    "$$\n",
    "where $\\pi \\in [0,1]$ is the mixture weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMixturePrior():\n",
    "    def __init__(self, pi=0.5, sigma1=torch.exp(torch.tensor(0)), sigma2=torch.exp(torch.tensor(-6)), device='cpu'):\n",
    "        self.device = device\n",
    "        self.pi = pi\n",
    "        self.mu = 0\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "    def prob(self, w, sigma):\n",
    "    \n",
    "        return (1 / (sigma * torch.sqrt(torch.tensor(2 * np.pi)))) * torch.exp(-0.5 * torch.pow((w - self.mu), 2) / torch.pow(sigma, 2))\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        prob1 = self.prob(w, self.sigma1)\n",
    "        prob2 = self.prob(w, self.sigma2)\n",
    "\n",
    "        return torch.log(self.pi * prob1 + (1 - self.pi) * prob2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualise the scale mixture prior below with $\\sigma_1 = \\exp(0)$ and $\\sigma_2 = \\exp(-6)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 0.5\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "def visualize_scale_mixture_components():\n",
    "    def show_lines():\n",
    "        pass\n",
    "    mix = ScaleMixturePrior(PI, SIGMA_1, SIGMA_2)\n",
    "    normal_1 = torch.distributions.Normal(0, SIGMA_1)\n",
    "    normal_2 = torch.distributions.Normal(0, SIGMA_2)\n",
    "    x_points = np.linspace(-5,5,10000)\n",
    "    d1 = np.array([torch.exp(normal_1.log_prob(torch.tensor(float(c)))).detach().cpu() for c in x_points])\n",
    "    d2 = np.array([torch.exp(normal_2.log_prob(torch.tensor(float(c)))).detach().cpu() for c in x_points])\n",
    "    d3 = np.array([torch.exp(mix.log_prob(torch.tensor(float(c)))).detach().cpu() for c in x_points])\n",
    "    print(max(d1), max(d2), max(d3))\n",
    "    plt.plot(x_points,d2,color=\"g\")\n",
    "    plt.plot(x_points,d3,color=\"r\")\n",
    "    plt.plot(x_points,d1,color=\"b\")\n",
    "    plt.legend([\"sigma2\", \"mix\", \"sigma1\"])\n",
    "    plt.ylim(0,0.5)\n",
    "\n",
    "    \n",
    "visualize_scale_mixture_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Bayesian linear layer for our Bayesian neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, pi=0.5, sigma1=torch.exp(torch.tensor(0)), sigma2=torch.tensor(0.3), device='cpu'):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "\n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-5, -4)) \n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(output_dim).uniform_(-5, -4))\n",
    "\n",
    "        # initialise priors\n",
    "        self.weight_prior = ScaleMixturePrior(pi, sigma1, sigma2, device=device)\n",
    "        self.bias_prior = ScaleMixturePrior(pi, sigma1, sigma2, device=device)\n",
    "\n",
    "        # initialise variational posteriors\n",
    "        self.weight_posterior = Gaussian(self.weight_mu, self.weight_rho, device=device)\n",
    "        self.bias_posterior = Gaussian(self.bias_mu, self.bias_rho, device=device)\n",
    "\n",
    "        self.log_prior = 0.0\n",
    "        self.log_variational_posterior = 0.0\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "        if sample:\n",
    "            w = self.weight_posterior.rsample()\n",
    "            b = self.bias_posterior.rsample()\n",
    "\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "            \n",
    "        else:\n",
    "            w = self.weight_posterior.mu\n",
    "            b = self.bias_posterior.mu\n",
    "\n",
    "            self.log_prior = 0.0\n",
    "            self.log_variational_posterior = 0.0\n",
    "\n",
    "        output = torch.mm(x, w) + b\n",
    "\n",
    "        # print(\"layer:\", w.device, b.device, output.device, self.log_prior.device, self.log_variational_posterior.device)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units1, hidden_units2, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.layer1 = BayesianLinearLayer(1, hidden_units1, device=device)\n",
    "        self.layer2 = BayesianLinearLayer(hidden_units1, hidden_units2, device=device)\n",
    "        self.layer3 = BayesianLinearLayer(hidden_units2, 2, device=device)\n",
    "\n",
    "        self.layers = [self.layer1, self.layer2, self.layer3]\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "        x = F.relu(self.layer1(x, sample))\n",
    "        x = F.relu(self.layer2(x, sample))\n",
    "        x = self.layer3(x, sample)\n",
    "\n",
    "        mu = x[:, 0]\n",
    "        rho = x[:, 1]\n",
    "\n",
    "        return mu, rho\n",
    "    \n",
    "    def inference(self, x, sample=True, n_samples=1):\n",
    "        # log_probs : (n_samples, batch_size)\n",
    "        mus = np.zeros((n_samples, x.size(0)))\n",
    "        sigmas = np.zeros((n_samples, x.size(0)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            mu, rho = self.forward(x, sample)\n",
    "            mus[i] = mu\n",
    "            sigmas[i] = self.get_sigma(rho)\n",
    "\n",
    "        expected_mu = torch.mean(mus, dim=0)\n",
    "        expected_sigma = (torch.mean((mus.pow(2) + sigmas.pow(2)), dim=0) - expected_mu.pow(2)).sqrt()\n",
    "    \n",
    "        return expected_mu, expected_sigma\n",
    "\n",
    "    def compute_log_prior(self):\n",
    "        model_log_prior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                model_log_prior += layer.log_prior\n",
    "        return model_log_prior\n",
    "\n",
    "    def compute_log_variational_posterior(self):\n",
    "        model_log_variational_posterior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                model_log_variational_posterior += layer.log_variational_posterior\n",
    "       \n",
    "        return model_log_variational_posterior\n",
    " \n",
    "    \n",
    "    def compute_NLL(self, mu, target, sigma):\n",
    "        loss_fn = torch.nn.GaussianNLLLoss(reduction='sum', eps=1e-6)\n",
    "        var = torch.pow(sigma, 2)\n",
    "        NLL = loss_fn(mu, target, var)\n",
    "        return NLL\n",
    "    \n",
    "    def get_sigma(self, rho):\n",
    "        return torch.log1p(torch.exp(rho))\n",
    "\n",
    "    def compute_ELBO(self, input, target, n_samples=1):\n",
    "\n",
    "        log_priors = torch.zeros(n_samples) \n",
    "        log_variational_posteriors = torch.zeros(n_samples) \n",
    "        NLLs = torch.zeros(n_samples) \n",
    "\n",
    "        for i in range(n_samples):\n",
    "            mu, rho = self.forward(input, sample=True)\n",
    "            sigma = self.get_sigma(rho)\n",
    "            log_priors[i] = self.compute_log_prior()\n",
    "            log_variational_posteriors[i] = self.compute_log_variational_posterior()\n",
    "            NLLs[i] = self.compute_NLL(mu, target, sigma)\n",
    "\n",
    "        log_prior = log_priors.mean(0)\n",
    "        log_variational_posterior = log_variational_posteriors.mean(0)\n",
    "        NLL = NLLs.mean(0)\n",
    "\n",
    "        loss = ((log_variational_posterior - log_prior) / NUM_BATCHES) + NLL\n",
    "\n",
    "        return loss, log_prior, log_variational_posterior, NLL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions ðŸ¤–\n",
    "\n",
    "def train_BNN(model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10, device='cpu'):\n",
    "    \n",
    "    if device == 'cpu':\n",
    "        print(\"Training on CPU\")\n",
    "    else:\n",
    "        print(\"Cuda available, training on GPU\")\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    log_priors = []\n",
    "    log_variational_posteriors = []\n",
    "    NLLs = []\n",
    "\n",
    "    val_losses = []\n",
    "    val_log_priors = []\n",
    "    val_log_variational_posteriors = []\n",
    "    val_NLLs = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_, y_ = x_.float().to(device), y_.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, log_prior, log_posterior, log_NLL = model.compute_ELBO(x_, y_)\n",
    "            \n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item()) \n",
    "            log_priors.append(log_prior.item())\n",
    "            log_variational_posteriors.append(log_posterior.item())\n",
    "            NLLs.append(log_NLL.item()) \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float().to(device), val_y.float().to(device)\n",
    "                \n",
    "                    val_loss, _ , _, _ = model.compute_ELBO(val_x, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, log_priors, log_variational_posteriors, NLLs, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_log_probs(log_priors, log_variational_posteriors, NLLs):\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "\n",
    "    ax[0].plot(log_priors, label='Train log prior')\n",
    "    ax[0].set_title('Train log prior')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Log prior')\n",
    "\n",
    "    ax[1].plot(log_variational_posteriors, label='Train log variational posterior', color='orange')\n",
    "    ax[1].set_title('Train log variational posterior')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Log variational posterior')\n",
    "\n",
    "    ax[2].plot(NLLs, label='Train NLL', color='green')\n",
    "    ax[2].set_title('Train NLL')\n",
    "    ax[2].set_xlabel('Iterations')\n",
    "    ax[2].set_ylabel('NLL')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "# create model\n",
    "BNN_model = BayesianNeuralNetwork(32, 128, device=device)\n",
    "BNN_model = BNN_model.to(device)\n",
    "optimizer = torch.optim.Adam(BNN_model.parameters(), lr=1e-4)\n",
    "\n",
    "losses, log_priors, log_variational_posteriors, NLLs, val_losses = train_BNN(BNN_model, optimizer, trainloader, valloader, epochs=10000, model_name='BNN', val_every_n_epochs=10, device=device)\n",
    "\n",
    "plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_probs(log_priors, log_variational_posteriors, NLLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"BNN.pt\")\n",
    "\n",
    "predictions = []\n",
    "stds = []\n",
    "\n",
    "for x_test, y_test in testloader:\n",
    "    x_test, y_test = x_test.float(), y_test.float()\n",
    "    with torch.no_grad():\n",
    "        # output = model(x_test, inference=False)\n",
    "        mu, rho = model.inference(x_test, sample=True, n_samples=10)\n",
    "        # mu = output[:,0]\n",
    "        # rho = output[:,1]\n",
    "        sigma = model.get_sigma(rho)\n",
    "        predictions.append(mu.cpu().detach().numpy())\n",
    "        stds.append(sigma.cpu().detach().numpy())\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1,1, figsize=(18,12))\n",
    "\n",
    "### plot mimo ###\n",
    "plt.plot(x_train, y_train, '.', label='Train data', color='orange')\n",
    "plt.plot(x_test[:,0], line, '--', label='true function', color='red')\n",
    "# plot test data\n",
    "plt.plot(x_test[:,0], y_test, '.', label='Test data', color='black')\n",
    "\n",
    "# plot predicitons with confidence intervals\n",
    "plt.plot(x_test[:,0], predictions[0], '-', label=f'BNN Prediction', linewidth=2)\n",
    "plt.fill_between(x_test[:,0], predictions[0] - 1.96*stds[0], predictions[0] + 1.96*stds[0], alpha=0.2, label=f'Confidence Interval')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing a Bayesian convolutional layer. Each value in the filter is represented by a Gaussian probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, device='cpu', pi=0.5, sigma1=torch.exp(torch.tensor(0)), sigma2=torch.tensor(0.3)):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        \n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        # use *kernel_size instead of writing (_, _, kernel_size, kernel_size)\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(in_channels, out_channels, *kernel_size).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(in_channels, out_channels, *kernel_size).uniform_(-5, -4))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_channels).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_channels).uniform_(-5, -4))\n",
    "\n",
    "        # initialise priors\n",
    "        self.weight_prior = ScaleMixturePrior(pi, sigma1, sigma2, device=device)\n",
    "        self.bias_prior = ScaleMixturePrior(pi, sigma1, sigma2, device=device)\n",
    "\n",
    "        # initialise variational posteriors\n",
    "        self.weight_posterior = Gaussian(self.weight_mu.permute(1,0,2,3).to(device), self.weight_rho.permute(1,0,2,3).to(device), device=device)\n",
    "        self.bias_posterior = Gaussian(self.bias_mu, self.bias_rho, device=device)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        if sample:\n",
    "            w = self.weight_posterior.rsample()\n",
    "            b = self.bias_posterior.rsample()\n",
    "\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "\n",
    "        else:\n",
    "            w = self.weight_posterior.mu\n",
    "            b = self.bias_posterior.mu\n",
    "\n",
    "            self.log_prior = 0.0\n",
    "            self.log_variational_posterior = 0.0\n",
    "\n",
    "        output = F.conv2d(x, w, b, self.stride, self.padding, self.dilation)\n",
    "\n",
    "        # print(\"layer:\", w.device, b.device, output.device, self.log_prior.device, self.log_variational_posterior.device)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a Gaussian CNN with 2 conv layers and 3 linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianConvNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units1, hidden_units2, channels1, channels2, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.conv1 = BayesianConvLayer(3, channels1, kernel_size=(3,3), padding=1, device=device)\n",
    "        self.conv2 = BayesianConvLayer(channels1, channels2, kernel_size=(3,3), padding=1, device=device)\n",
    "        self.layer1 = BayesianLinearLayer(channels2*32*32, hidden_units1, device=device)\n",
    "        self.layer2 = BayesianLinearLayer(hidden_units1, hidden_units2, device=device)\n",
    "        self.layer3 = BayesianLinearLayer(hidden_units2, 10, device=device)\n",
    "\n",
    "        \n",
    "        self.layers = [self.conv1, self.conv2, self.layer1, self.layer2, self.layer3]\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "        # print(\"x\", x.device)\n",
    "        x = F.relu(self.conv1(x, sample))\n",
    "        x = F.relu(self.conv2(x, sample))\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = F.relu(self.layer1(x, sample))\n",
    "        x = F.relu(self.layer2(x, sample))\n",
    "        x = self.layer3(x, sample)\n",
    "        probs = F.log_softmax(x, dim=1)\n",
    "        x = torch.argmax(probs, dim=1)\n",
    "\n",
    "        return x, probs\n",
    "    \n",
    "    def inference(self, x, sample=True, n_samples=1, n_classes=10):\n",
    "        # log_probs : (n_samples, batch_size, n_classes)\n",
    "        log_probs = np.zeros((n_samples, x.size(0), n_classes))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            pred, probs = self.forward(x, sample)\n",
    "            log_probs[i] = probs.cpu().detach().numpy()\n",
    "\n",
    "        mean_log_probs = log_probs.mean(0)\n",
    "        mean_predictions = np.argmax(mean_log_probs, axis=1)\n",
    "\n",
    "        return mean_predictions, mean_log_probs\n",
    "\n",
    "    def compute_log_prior(self):\n",
    "        model_log_prior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (BayesianLinearLayer, BayesianConvLayer)):\n",
    "                model_log_prior += layer.log_prior\n",
    "        return model_log_prior\n",
    "\n",
    "    def compute_log_variational_posterior(self):\n",
    "        model_log_variational_posterior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (BayesianLinearLayer, BayesianConvLayer)):\n",
    "                model_log_variational_posterior += layer.log_variational_posterior\n",
    "        return model_log_variational_posterior\n",
    "    \n",
    "    def compute_NLL(self, pred, target):\n",
    "        loss_fn = torch.nn.NLLLoss(reduction='sum')\n",
    "        NLL = loss_fn(pred, target)\n",
    "        return NLL\n",
    "    \n",
    "    def get_sigma(self, rho):\n",
    "        return torch.log1p(torch.exp(rho))\n",
    "\n",
    "    def compute_ELBO(self, input, target, n_samples=1):\n",
    "        log_priors = torch.zeros(n_samples) \n",
    "        log_variational_posteriors = torch.zeros(n_samples) \n",
    "        NLLs = torch.zeros(n_samples) \n",
    "\n",
    "        for i in range(n_samples):\n",
    "            pred, probs = self.forward(input, sample=True)\n",
    "            log_priors[i] = self.compute_log_prior()\n",
    "            log_variational_posteriors[i] = self.compute_log_variational_posterior()\n",
    "            NLLs[i] = self.compute_NLL(probs, target)\n",
    "\n",
    "        log_prior = log_priors.mean(0)\n",
    "        log_variational_posterior = log_variational_posteriors.mean(0)\n",
    "        NLL = NLLs.mean(0)\n",
    "\n",
    "        loss = ((log_variational_posterior - log_prior) / NUM_BATCHES) + NLL\n",
    " \n",
    "        return loss, log_prior, log_variational_posterior, NLL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load/download CIFAR10\n",
    "# https://github.com/kuangliu/pytorch-cifar/issues/19 normalisation values\n",
    "transform = transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "CIFAR_traindata = torchvision.datasets.CIFAR10(root=\"../data/\", train=True, transform = transform, download=False)\n",
    "CIFAR_train, CIFAR_val = torch.utils.data.random_split(CIFAR_traindata, [int(len(CIFAR_traindata)*0.9), int(len(CIFAR_traindata)*0.1)])\n",
    "CIFAR_test = torchvision.datasets.CIFAR10(root=\"../data/\", train=False, transform = transform, download=False)\n",
    "\n",
    "CIFAR_trainloader = DataLoader(CIFAR_train, batch_size=500, shuffle=True, pin_memory=True)\n",
    "CIFAR_valloader = DataLoader(CIFAR_val, batch_size=500, shuffle=True, pin_memory=True)\n",
    "CIFAR_testloader = DataLoader(CIFAR_test, batch_size=500, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions ðŸ¤–\n",
    "\n",
    "def train_BNN_classification(model, optimizer, trainloader, valloader, epochs=500, model_name='C_BNN', val_every_n_epochs=10, device='cpu'):\n",
    "    \n",
    "    if device == 'cpu':\n",
    "        print(\"Training on CPU\")\n",
    "    else:\n",
    "        print(\"Cuda available, training on GPU\")\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    log_priors = []\n",
    "    log_variational_posteriors = []\n",
    "    NLLs = []\n",
    "\n",
    "    val_losses = []\n",
    "    val_log_priors = []\n",
    "    val_log_variational_posteriors = []\n",
    "    val_NLLs = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_, y_ = x_.float().to(device), y_.type(torch.LongTensor).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, log_prior, log_posterior, log_NLL = model.compute_ELBO(x_, y_)\n",
    "            # print(loss)\n",
    "\n",
    "            # print(loss.device, log_prior.device, log_posterior.device, log_NLL.device)\n",
    "            \n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item()) \n",
    "            log_priors.append(log_prior.item())\n",
    "            log_variational_posteriors.append(log_posterior.item())\n",
    "            NLLs.append(log_NLL.item()) \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float().to(device), val_y.type(torch.LongTensor).to(device)\n",
    "                \n",
    "                    val_loss, _ , _, _ = model.compute_ELBO(val_x, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, log_priors, log_variational_posteriors, NLLs, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_log_probs(log_priors, log_variational_posteriors, NLLs):\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "\n",
    "    ax[0].plot(log_priors, label='Train log prior')\n",
    "    ax[0].set_title('Train log prior')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Log prior')\n",
    "\n",
    "    ax[1].plot(log_variational_posteriors, label='Train log variational posterior', color='orange')\n",
    "    ax[1].set_title('Train log variational posterior')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Log variational posterior')\n",
    "\n",
    "    ax[2].plot(NLLs, label='Train NLL', color='green')\n",
    "    ax[2].set_title('Train NLL')\n",
    "    ax[2].set_xlabel('Iterations')\n",
    "    ax[2].set_ylabel('NLL')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "\n",
    "# create model\n",
    "BNN_model = BayesianConvNeuralNetwork(hidden_units1=32, hidden_units2=128, channels1=32, channels2=64, device=device)\n",
    "BNN_model = BNN_model.to(device)\n",
    "optimizer = torch.optim.Adam(BNN_model.parameters(), lr=1e-4)\n",
    "\n",
    "losses, log_priors, log_variational_posteriors, NLLs, val_losses = train_BNN_classification(BNN_model, optimizer, CIFAR_trainloader, CIFAR_valloader, epochs=30, model_name='C_BNN', val_every_n_epochs=5, device=device)\n",
    "\n",
    "plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_probs(log_priors, log_variational_posteriors, NLLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"C_BNN.pt\")\n",
    "\n",
    "preds = []\n",
    "log_probs = []\n",
    "targets = []\n",
    "\n",
    "for x_test, y_test in CIFAR_testloader:\n",
    "    x_test, y_test = x_test.float().to(device), y_test.type(torch.LongTensor).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred, probs = model.inference(x_test, sample=True, n_samples=10)\n",
    "        preds.extend(pred)\n",
    "        log_probs.extend(probs)\n",
    "        targets.extend(y_test.cpu().detach().numpy())\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "\n",
    "accuracy = (np.array(preds) == np.array(targets)).sum() / len(preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
