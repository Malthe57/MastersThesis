{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/toydata/train_data.csv')\n",
    "df_val = pd.read_csv('../data/toydata/val_data.csv')\n",
    "df_test = pd.read_csv('../data/toydata/test_data.csv')\n",
    "\n",
    "x_train, y_train = df_train['x'].values, df_train['y'].values\n",
    "x_val, y_val = df_val['x'].values, df_val['y'].values\n",
    "x_test, y_test = df_test['x'].values, df_test['y'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \"\"\"Custom toy dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    return torch.tensor(x)[:,None], torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed workers for reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(0)\n",
    "\n",
    "traindata = ToyDataset(x_train, y_train)\n",
    "trainloader = DataLoader(traindata, batch_size=500, shuffle=True, collate_fn=collate_fn, drop_last=True, pin_memory=True)\n",
    "\n",
    "valdata = ToyDataset(x_val, y_val)\n",
    "valloader = DataLoader(valdata, batch_size=500, shuffle=False, collate_fn=collate_fn, drop_last=True, pin_memory=True)\n",
    "\n",
    "testdata = ToyDataset(x_test, y_test)\n",
    "testloader = DataLoader(testdata, batch_size=len(y_test), shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "NUM_BATCHES = len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMixturePrior():\n",
    "    def __init__(self, pi=0.5, sigma1=torch.exp(torch.tensor(0)), sigma2=torch.exp(torch.tensor(-6))):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pi = pi\n",
    "        self.mu = 0\n",
    "        self.sigma1 = sigma1.to(self.device)\n",
    "        self.sigma2 = sigma2.to(self.device)\n",
    "\n",
    "    def prob(self, w, sigma):\n",
    "        return (1 / (sigma * torch.sqrt(torch.tensor(2 * np.pi)))) * torch.exp(-0.5 * torch.pow((w - self.mu), 2) / torch.pow(sigma, 2))\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        prob1 = self.prob(w, self.sigma1)\n",
    "        prob2 = self.prob(w, self.sigma2)\n",
    "\n",
    "        return torch.log(self.pi * prob1 + (1 - self.pi) * prob2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian():\n",
    "    def __init__(self, mu, rho):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mu = mu.to(self.device)\n",
    "        self.rho = rho.to(self.device)\n",
    "        self.normal = torch.distributions.Normal(mu, self.sigma)\n",
    "        \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def rsample(self):\n",
    "        return self.normal.rsample().to(self.device)\n",
    "\n",
    "    def log_prob(self, w):\n",
    "        return self.normal.log_prob(w).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # initialise mu and rho parameters so they get updated in backpropagation\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(input_dim, output_dim).uniform_(-2, -1)) \n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(output_dim).uniform_(-2, -1))\n",
    "\n",
    "        # initialise priors\n",
    "        self.weight_prior = ScaleMixturePrior()\n",
    "        self.bias_prior = ScaleMixturePrior()\n",
    "\n",
    "        # initialise variational posteriors\n",
    "        self.weight_posterior = Gaussian(self.weight_mu, self.weight_rho)\n",
    "        self.bias_posterior = Gaussian(self.bias_mu, self.bias_rho)\n",
    "\n",
    "        self.log_prior = 0.0\n",
    "        self.log_variational_posterior = 0.0\n",
    "\n",
    "    def forward(self, x, inference=False):\n",
    "\n",
    "        if inference:\n",
    "            w = self.weight_mu\n",
    "            b = self.bias_mu\n",
    "\n",
    "            self.log_prior = 0.0\n",
    "            self.log_variational_posterior = 0.0\n",
    "\n",
    "        else:\n",
    "            w = self.weight_posterior.rsample()\n",
    "            b = self.bias_posterior.rsample()\n",
    "\n",
    "            self.log_prior = self.weight_prior.log_prob(w) + self.bias_prior.log_prob(b)\n",
    "            self.log_variational_posterior = self.weight_posterior.log_prob(w) + self.bias_posterior.log_prob(b)\n",
    "\n",
    "        output = torch.mm(x, w) + b\n",
    "\n",
    "        print(\"layer:\", w.device, b.device, output.device, self.log_prior.device, self.log_variational_posterior.device)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units1, hidden_units2, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.layer1 = BayesianLinearLayer(1, hidden_units1)\n",
    "        self.layer2 = BayesianLinearLayer(hidden_units1, hidden_units2)\n",
    "        self.layer3 = BayesianLinearLayer(hidden_units2, 2)\n",
    "\n",
    "        self.layers = [self.layer1, self.layer2, self.layer3]\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, inference=False):\n",
    "        print(\"x\", x.device)\n",
    "        x = F.relu(self.layer1(x, inference))\n",
    "        x = F.relu(self.layer2(x, inference))\n",
    "        x = self.layer3(x, inference)\n",
    "\n",
    "        mu = x[:, 0]\n",
    "        rho = x[:, 1]\n",
    "        print(\"mu\", mu.device, \"rho\", rho.device, \"x\", x.device)\n",
    "        return mu, rho\n",
    "\n",
    "    def compute_log_prior(self):\n",
    "        model_log_prior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                model_log_prior += layer.log_prior\n",
    "        print(\"model_log_prior\", model_log_prior.device)\n",
    "        return model_log_prior\n",
    "\n",
    "    def compute_log_variational_posterior(self):\n",
    "        model_log_variational_posterior = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BayesianLinearLayer):\n",
    "                model_log_variational_posterior += layer.log_variational_posterior\n",
    "        print(\"model_log_variational_posterior\", model_log_variational_posterior.device)\n",
    "        return model_log_variational_posterior\n",
    "\n",
    "    def compute_NLL(self, mu, target, sigma):\n",
    "        loss_fn = torch.nn.GaussianNLLLoss(reduction='sum', eps=1e-6)\n",
    "        var = torch.pow(sigma, 2)\n",
    "\n",
    "        NLL = loss_fn(mu, target, var)\n",
    "        print(\"NLL\", NLL.device)\n",
    "        return NLL\n",
    "    \n",
    "    def get_sigma(self, rho):\n",
    "        print(\"sigma\", torch.log1p(torch.exp(rho)).device)\n",
    "        return torch.log1p(torch.exp(rho))\n",
    "\n",
    "    def compute_ELBO(self, input, target, n_samples=1):\n",
    "\n",
    "        log_priors = torch.zeros(n_samples).to(self.device)\n",
    "        log_variational_posteriors = torch.zeros(n_samples).to(self.device)\n",
    "        NLLS = torch.zeros(n_samples).to(self.device)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            mu, rho = self(input, inference=False)\n",
    "            sigma = self.get_sigma(rho)\n",
    "            log_priors[i] = self.compute_log_prior()\n",
    "            log_variational_posteriors[i] = self.compute_log_variational_posterior()\n",
    "            NLLS[i] = self.compute_NLL(mu, target, sigma)\n",
    "\n",
    "        log_prior = log_priors.mean(0).to(self.device)\n",
    "        log_variational_posterior = log_variational_posteriors.mean(0).to(self.device)\n",
    "        NLL = NLLS.mean(0).to(self.device) \n",
    "\n",
    "        loss = ((log_variational_posterior - log_prior) / NUM_BATCHES) + NLL\n",
    "\n",
    "        loss = loss.to(self.device)\n",
    "\n",
    "        print(loss, log_prior, log_variational_posterior, NLL)\n",
    " \n",
    "        return loss, log_prior, log_variational_posterior, NLL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions 🤖\n",
    "\n",
    "def train_BNN(model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10, device='cpu'):\n",
    "\n",
    "    losses = []\n",
    "    log_priors = []\n",
    "    log_variational_posteriors = []\n",
    "    NLLs = []\n",
    "\n",
    "    val_losses = []\n",
    "    val_log_priors = []\n",
    "    val_log_variational_posteriors = []\n",
    "    val_NLLs = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        \n",
    "        for x_, y_ in trainloader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            x_, y_ = x_.float().to(device), y_.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, log_prior, log_posterior, log_NLL = model.compute_ELBO(x_, y_)\n",
    "            # print(loss)\n",
    "\n",
    "            print(loss.device, log_prior.device, log_posterior.device, log_NLL.device)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item()) \n",
    "            log_priors.append(log_prior.item())\n",
    "            log_variational_posteriors.append(log_posterior.item())\n",
    "            NLLs.append(log_NLL.item()) \n",
    "\n",
    "        if (e+1) % val_every_n_epochs == 0:\n",
    "            model.eval()\n",
    "\n",
    "            val_loss_list = []\n",
    "            with torch.no_grad():\n",
    "                for val_x, val_y in valloader:\n",
    "                    val_x, val_y = val_x.float().to(device), val_y.float().to(device)\n",
    "                \n",
    "                    val_loss, _ , _, _ = model.compute_ELBO(val_x, val_y)\n",
    "                    val_loss_list.append(val_loss.item())\n",
    "\n",
    "            val_losses.extend(val_loss_list)\n",
    "            mean_val_loss = np.mean(val_loss_list)\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_val_loss = mean_val_loss\n",
    "                torch.save(model, f'{model_name}.pt')\n",
    "            # print(f\"Mean validation loss at epoch {e}: {mean_val_loss}\")\n",
    "\n",
    "    return losses, log_priors, log_variational_posteriors, NLLs, val_losses\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "    ax[0].plot(losses, label='Train loss')\n",
    "    ax[0].set_title('Train loss')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(val_losses, label='Validation loss', color='orange')\n",
    "    ax[1].set_title('Validation loss')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_log_probs(log_priors, log_variational_posteriors, NLLs):\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "\n",
    "    ax[0].plot(log_priors, label='Train log prior')\n",
    "    ax[0].set_title('Train log prior')\n",
    "    ax[0].set_xlabel('Iterations')\n",
    "    ax[0].set_ylabel('Log prior')\n",
    "\n",
    "    ax[1].plot(log_variational_posteriors, label='Train log variational posterior', color='orange')\n",
    "    ax[1].set_title('Train log variational posterior')\n",
    "    ax[1].set_xlabel('Iterations')\n",
    "    ax[1].set_ylabel('Log variational posterior')\n",
    "\n",
    "    ax[2].plot(NLLs, label='Train NLL', color='green')\n",
    "    ax[2].set_title('Train NLL')\n",
    "    ax[2].set_xlabel('Iterations')\n",
    "    ax[2].set_ylabel('NLL')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e407f54b9a41d7838ece17dd88734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x cuda:0\n",
      "layer: cuda:0 cuda:0 cuda:0 cuda:0 cuda:0\n",
      "layer: cuda:0 cuda:0 cuda:0 cuda:0 cuda:0\n",
      "layer: cuda:0 cuda:0 cuda:0 cuda:0 cuda:0\n",
      "mu cuda:0 rho cuda:0 x cuda:0\n",
      "sigma cuda:0\n",
      "model_log_prior cuda:0\n",
      "model_log_variational_posterior cuda:0\n",
      "NLL cuda:0\n",
      "tensor(2342.9048, device='cuda:0', grad_fn=<AddBackward0>) tensor(-6897.3286, device='cuda:0', grad_fn=<MeanBackward1>) tensor(713.8870, device='cuda:0', grad_fn=<MeanBackward1>) tensor(440.1008, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "cuda:0 cuda:0 cuda:0 cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m BNN_model \u001b[38;5;241m=\u001b[39m BNN_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(BNN_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m losses, log_priors, log_variational_posteriors, NLLs, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_BNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBNN_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plot_loss(losses, val_losses)\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mtrain_BNN\u001b[1;34m(model, optimizer, trainloader, valloader, epochs, model_name, val_every_n_epochs, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdevice, log_prior\u001b[38;5;241m.\u001b[39mdevice, log_posterior\u001b[38;5;241m.\u001b[39mdevice, log_NLL\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 33\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \n\u001b[0;32m     36\u001b[0m log_priors\u001b[38;5;241m.\u001b[39mappend(log_prior\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yucheng\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\optim\\adam.py:520\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    517\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m    523\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create model\n",
    "BNN_model = BayesianNeuralNetwork(32, 128, device=device)\n",
    "BNN_model = BNN_model.to(device)\n",
    "optimizer = torch.optim.Adam(BNN_model.parameters(), lr=3e-5)\n",
    "\n",
    "losses, log_priors, log_variational_posteriors, NLLs, val_losses = train_BNN(BNN_model, optimizer, trainloader, valloader, epochs=500, model_name='BNN', val_every_n_epochs=10, device=device)\n",
    "\n",
    "plot_loss(losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_priors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_log_probs(\u001b[43mlog_priors\u001b[49m, log_variational_posteriors, NLLs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_priors' is not defined"
     ]
    }
   ],
   "source": [
    "plot_log_probs(log_priors, log_variational_posteriors, NLLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
